{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdd90229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72dce209",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = requests.get(\"https://www.google.com/search?q=redhat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "860ed6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2c9862d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58c39dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_openai_key = \"Enter your Key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a52304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775b67bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13864d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4d7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7d1a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5d68f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_endpoint = 'https://nubela.co/proxycurl/api/v2/linkedin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7fada214",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_dic = {'Authorization': 'Bearer ' + api_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "dafb63e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "myparams = {\n",
    "    'url': 'https://www.linkedin.com/in/vimaldaga/',\n",
    "    'fallback_to_cache': 'on-error',\n",
    "    'use_cache': 'if-present',\n",
    "    'skills': 'include',\n",
    "    'inferred_salary': 'include',\n",
    "    'personal_email': 'include',\n",
    "    'personal_contact_number': 'include',\n",
    "    'twitter_profile_id': 'include',\n",
    "    'facebook_profile_id': 'include',\n",
    "    'github_profile_id': 'include',\n",
    "    'extra': 'include',\n",
    "    'headline': 'include',\n",
    "    'certifications': 'include'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1017aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\n",
    "        url=api_endpoint,\n",
    "        params=myparams,\n",
    "        headers=header_dic\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a241009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79b23e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "myprofiledata = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0c0d3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vimal Daga'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myprofiledata['full_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4699775e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Aloha, I am Vimal Daga, known as an Technologist & also a Technology Motivational Speaker, Sr. IT Consultant & Corporate Trainer having Expertize in Latest and High-End Technologies like Machine Learning, Artificial Intelligence, Deep Learning,IoT, NLP, Splunk, PingFederate, Delphix, AppDynamics, Docker, DevOps, AWS, Cloud Computing, Big Data Analytics Dollar Universe..\\n\\nEmail: vdaga@lwindia.com\\n\\nI am an IT Enthusiast, who is passionate about exploring all the latest technologies from research perspective. I inspire, train and provide Consulting Services to all the companies willing to work, migrate or use high end IT Technologies and also help MNC's to find the right approach to get best ROI.\\n◆ MY BELIEF\\nIn my own words – “No technology is challenging or difficult or complex as the world says to us or approaches to us since it is MAN-MADE. You just need the right path or approach to understand the technology and take it further as per one’s requirement”. Right thinking or positive thinking that “Yes I Can Do It” can change one’s life drastically.\\n\\n◆ MY WHAT\\nI train and provide consultancy to all the Top Level Management including the Directors, CTO’s, Project Leads and also Senior IT executives working on technologies or willing to understand whether the new technology which they are willing to adapt is the right one. It helps them in enhancing their technical skills and also ability to handle complex project based environment.\\n\\n◆ MY TECHNOLOGIES\\n•      Machine Learning, AI, DL, NLP\\n•\\tDevOps – Chef, Puppet, Ansible, Jenkins, Docker\\n•\\tOps Intelligence & Other Tools – Splunk, PingFederate, Delphix, AppDynamics\\n•\\tCloud Computing – AWS, OpenStack, \\n•\\tBigData Analytics – Hadoop, Spark, Mahout, Cassandra\\n•\\tRedHat, Cisco, Security and Many more\\n\\n◆ MY SUCCESSFUL CLIENTS\\nWells Fargo, JP Morgan Chase, Bank of America, GE, CA Technologies, Dell, TCS, Deloitte, PwC, HP, Wipro, IBM, Samsung R&D, Ericsson and many more…\\n\\nLET'S CONNECT\\nI am easy to reach M-F at 0091.982.912.5960\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myprofiledata['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b975de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fdbfcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901ded23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a0bebbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_endpoint = 'https://nubela.co/proxycurl/api/v2/linkedin'\n",
    "header_dic = {'Authorization': 'Bearer ' + api_key}\n",
    "# Custom Tool\n",
    "def lw_scrape_linkedin(myurl):\n",
    "    myparams = {\n",
    "    'url': 'https://www.linkedin.com/in/vimaldaga/',\n",
    "    'fallback_to_cache': 'on-error',\n",
    "    'use_cache': 'if-present',\n",
    "    'skills': 'include',\n",
    "    'inferred_salary': 'include',\n",
    "    'personal_email': 'include',\n",
    "    'personal_contact_number': 'include',\n",
    "    'twitter_profile_id': 'include',\n",
    "    'facebook_profile_id': 'include',\n",
    "    'github_profile_id': 'include',\n",
    "    'extra': 'include',\n",
    "    'headline': 'include',\n",
    "    'certifications': 'include'\n",
    "    }\n",
    "    response = requests.get(\n",
    "        url=api_endpoint,\n",
    "        params=myparams,\n",
    "        headers=header_dic\n",
    "    )\n",
    "    myprofiledata = response.json()\n",
    "    \n",
    "    return  myprofiledata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca5c7ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = lw_scrape_linkedin('https://www.linkedin.com/in/vimaldaga/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8067629f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'World Record Holder | TEDx Speaker | Philanthropist | Sr. Principal Consultant | Entrepreneur | Founder LW Informatics'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bdb31c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'World Record Holder '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['headline'].split(\"|\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ea771d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'public_identifier': 'vimaldaga',\n",
       " 'profile_pic_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/person/vimaldaga/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T082517Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=045bf9031be95342a29638ba9114f3336cef6fbd71d56891fc7a2e496231f134',\n",
       " 'background_cover_image_url': None,\n",
       " 'first_name': 'Vimal',\n",
       " 'last_name': 'Daga',\n",
       " 'full_name': 'Vimal Daga',\n",
       " 'follower_count': None,\n",
       " 'occupation': 'Sr. Machine Learning / Deep Learning / Data Scientist / NLP Consultant and Researcher at LinuxWorld Informatics Pvt Ltd',\n",
       " 'headline': 'World Record Holder | TEDx Speaker | Philanthropist | Sr. Principal Consultant | Entrepreneur | Founder LW Informatics',\n",
       " 'summary': \"Aloha, I am Vimal Daga, known as an Technologist & also a Technology Motivational Speaker, Sr. IT Consultant & Corporate Trainer having Expertize in Latest and High-End Technologies like Machine Learning, Artificial Intelligence, Deep Learning,IoT, NLP, Splunk, PingFederate, Delphix, AppDynamics, Docker, DevOps, AWS, Cloud Computing, Big Data Analytics Dollar Universe..\\n\\nEmail: vdaga@lwindia.com\\n\\nI am an IT Enthusiast, who is passionate about exploring all the latest technologies from research perspective. I inspire, train and provide Consulting Services to all the companies willing to work, migrate or use high end IT Technologies and also help MNC's to find the right approach to get best ROI.\\n◆ MY BELIEF\\nIn my own words – “No technology is challenging or difficult or complex as the world says to us or approaches to us since it is MAN-MADE. You just need the right path or approach to understand the technology and take it further as per one’s requirement”. Right thinking or positive thinking that “Yes I Can Do It” can change one’s life drastically.\\n\\n◆ MY WHAT\\nI train and provide consultancy to all the Top Level Management including the Directors, CTO’s, Project Leads and also Senior IT executives working on technologies or willing to understand whether the new technology which they are willing to adapt is the right one. It helps them in enhancing their technical skills and also ability to handle complex project based environment.\\n\\n◆ MY TECHNOLOGIES\\n•      Machine Learning, AI, DL, NLP\\n•\\tDevOps – Chef, Puppet, Ansible, Jenkins, Docker\\n•\\tOps Intelligence & Other Tools – Splunk, PingFederate, Delphix, AppDynamics\\n•\\tCloud Computing – AWS, OpenStack, \\n•\\tBigData Analytics – Hadoop, Spark, Mahout, Cassandra\\n•\\tRedHat, Cisco, Security and Many more\\n\\n◆ MY SUCCESSFUL CLIENTS\\nWells Fargo, JP Morgan Chase, Bank of America, GE, CA Technologies, Dell, TCS, Deloitte, PwC, HP, Wipro, IBM, Samsung R&D, Ericsson and many more…\\n\\nLET'S CONNECT\\nI am easy to reach M-F at 0091.982.912.5960\",\n",
       " 'country': 'IN',\n",
       " 'country_full_name': 'India',\n",
       " 'city': 'Bengaluru',\n",
       " 'state': 'Karnataka',\n",
       " 'experiences': [{'starts_at': {'day': 1, 'month': 10, 'year': 2009},\n",
       "   'ends_at': None,\n",
       "   'company': 'LinuxWorld Informatics Pvt. Ltd.',\n",
       "   'company_linkedin_profile_url': None,\n",
       "   'title': 'Sr. Splunk Consultant and Corporate Trainer',\n",
       "   'description': 'Splunk Consulting, Architect, SIEM, Administration, Developing and Training \\n – PwC, WellsFargo, JP Morgan Chase, Deloitte,  many more\\n\\nMore Than 4 years experience in Splunk Entreprise. Deploying large data analytics architectures for customers mainly in the Banking section, Governments and Security companies. Consult and Solutions to Over 30 Splunk projects and over 40 Corporate Batch Trained\\n\\nIntegrating Splunk with Arcsight and Splunk for Security and Risk Analysis including Information Asset Management and SIEM.\\n\\n•       Architecture design and implementation of Splunk solution\\n•       Platform optimization\\n•       Design and implement indexers and search heads Clusters (+migrations)\\n•\\tImplementing distributed Splunk and ES\\n•\\tConfiguring Splunk heavy forwarders to receive logs using push or pull\\n•\\tWriting complex correlation searches for data analysis and notable events\\n•\\tWriting optimized searches for realtime and historical data analysis\\n•\\tWriting custom apps for field extractions and data normalization\\n•\\tDeveloping and deploying customized configuration apps for distributed Splunk\\n•\\tWriting field extractions for desired fields out of proprietary application data\\n•\\tCreating reports and correlated alerts for stake holders about application behavior and anomalies\\n•\\tData aggregation and summarization by summary indexing and acceleration \\n•\\tWriting transaction searches and data normalization\\n•\\tIntegrating Splunk with AD servers for ldap authentications or SAML PING FEDERATE, DBs for enrichments, Hadoop servers for parallel processing using Hadoop Connect\\n•\\tUpgrading stand-alone Splunk to distributed Splunk implementation\\n•\\tWriting custom inputs, props and transforms to have correct event-breaks, field extractions and transformations\\n•\\tStrategizing and writing custom configurations for data routing and selective indexing for the location wise forwarders\\n•\\tDeveloping interactive dashboards, reports and optimized search queries for users',\n",
       "   'location': None,\n",
       "   'logo_url': None},\n",
       "  {'starts_at': {'day': 1, 'month': 4, 'year': 2014},\n",
       "   'ends_at': None,\n",
       "   'company': 'LinuxWorld Informatics Pvt Ltd',\n",
       "   'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/',\n",
       "   'title': 'Microservices Architect and Corporate Trainer',\n",
       "   'description': '•\\tMicroservices Spring boot/Spring cloud, Dropwizard\\n•\\tExpertise in Java/ J2EE, Microservices , Spring Boot, Spring Cloud, EJB, terms of designing and implementing systems.\\n•\\tmodel and build scalable Microservices components/ APIs \\n•\\tmodel object for no sql DB \\n•\\tmodel and build integration flows on various API development platform \\n•\\tperform pair programming \\n•\\tbuild test scripts for Microservices components \\n•\\tDecompose an existing monolithic into microservices archtecture \\n•\\tcreated microservices architecture for new development \\n•\\tDefine, develop and manage API lifecycle or PaaS \\n•\\tDefine, develop components compliant wth Microservices architecture patterns \\n•\\tDefine and model cloud based persistence storage \\n•\\tIntegrate with cloud based storage, on-prem applcations / integration layer \\n•\\tDefine, develop runtime models for Microservices architecture \\n•\\tDefine, develop highly scalable deployment model for Microservices\\n•\\tImplement microservices on Docker Containerized environment',\n",
       "   'location': None,\n",
       "   'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T082521Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=68d18721868144c85491fd670ba0d78a60f6c8bd19c3e6c876d085dc5bbb81a4'},\n",
       "  {'starts_at': {'day': 1, 'month': 2, 'year': 2014},\n",
       "   'ends_at': None,\n",
       "   'company': 'LinuxWorld Informatics Pvt Ltd',\n",
       "   'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/',\n",
       "   'title': 'Appdynamics Consultant and Trainer',\n",
       "   'description': 'AWS Cloud Principal Consultant, Architect, Integrator and Corporate Trainer\\n– WellsFargo, JP Morgan Chase  many more\\n\\n•\\tManaging and Monitoring various metrics to analyze performance of application hosted on PaaS platform using Appdynamics for microservices\\n•\\tMonitor multi-tier architecture using AppDynamics Platform deployed on Cloud\\n•\\tMonitoring slow queries and finding bottlenecks\\n•\\tInstall App Agent & Machine agent\\n•\\tComplex Application monitoring in Appdynamics \\n•\\tCreate Application Dashboard and manage Transaction Snapshot / Scorecard and Diagnostic session\\n•\\tCodelevel issue repot and Monitor and Analyze\\n•\\tConfiguring Dashboards and Charting Data and Alert Log / Troubleshooting\\n•\\tCreating and Managing Reports and Portal Administration\\n•\\tAccount Details and User Management in appdynamics',\n",
       "   'location': None,\n",
       "   'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T082521Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=68d18721868144c85491fd670ba0d78a60f6c8bd19c3e6c876d085dc5bbb81a4'},\n",
       "  {'starts_at': {'day': 1, 'month': 10, 'year': 2013},\n",
       "   'ends_at': None,\n",
       "   'company': 'LinuxWorld Informatics Pvt Ltd',\n",
       "   'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/',\n",
       "   'title': 'Sr. Docker CI/CD Architect, Consultant and Corporate Trainer',\n",
       "   'description': 'Dcoker Container Consulting, Architect, Administration and Training \\n – Samsung R&D, Dell EMC, Verizon, NTT,  GE, CA Technology, Oracle, IBM, Amazon, Wipro, TCS, many more\\n\\nTraining and Consulting to Team Leads, CoE, Sr. Engineers for container related technologies like Docker with Cluster and Orchestration Engine Swarm, Kubernetes, Mesosphere and entire CI/CD cycle with containers\\nTrained Cloud DevOps Engineer to raise the bar for a high-performing team that delivers large scale solutions on the cloud. Fast-moving organization to transform a large enterprise onto the next generation of Cloud.\\n\\nAutomating large scale deployments with modern configuration and deployment management systems on AWS using Docker Containers Environment with ECS and managed through Code Pipeline.\\nDelivered container based deployments using Docker and can offer solutions to the complexities of Orchestration and will drive the company towards delivering containers in production.\\n\\n•\\tExtensive experience in Continuous Integration tool Bamboo/Jenkins/Circle CI with Integration with Docker\\n•\\tExperience in automation of the deployment and configuration of the virtualized infrastructure and the entire software stack and network stack through Docker\\n•\\tExtensive experience in build configurations using Build Management tool Docker for applications in various technologies including Python, Ruby, Scala, Java and many more\\n•\\tWorked and Trained on Docker Compose to auto High-End build pipeline\\n\\nExpert on Ansible/ Chef / Puppet to provisioning Docker task and in working with scripting Languages to configuring and maintaining CI/CD environment and automate AGILE Projects',\n",
       "   'location': None,\n",
       "   'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T082521Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=68d18721868144c85491fd670ba0d78a60f6c8bd19c3e6c876d085dc5bbb81a4'},\n",
       "  {'starts_at': {'day': 1, 'month': 2, 'year': 2013},\n",
       "   'ends_at': None,\n",
       "   'company': 'LinuxWorld Informatics Pvt Ltd',\n",
       "   'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/',\n",
       "   'title': 'Centrify IDaaS Consultant, Integrator and Corporate Trainer',\n",
       "   'description': 'Centrify Principal Consultant, Architect, Integrator and Corporate Trainer\\n–  IBM and many more\\n\\nIntegrated and Implement  Centrify identity management, security, SaaS single sign-on, Identity as a Service (IDaaS), mobile device management, mobile authentication, privileged access management, multi-factor authentication (MFA), privileged identity management, and identity and access management\\n\\nImplement POC of Centrify Server Suite with multiple vendor with different use cases',\n",
       "   'location': None,\n",
       "   'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T082521Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=68d18721868144c85491fd670ba0d78a60f6c8bd19c3e6c876d085dc5bbb81a4'},\n",
       "  {'starts_at': {'day': 1, 'month': 3, 'year': 2012},\n",
       "   'ends_at': None,\n",
       "   'company': 'LinuxWorld Informatics Pvt Ltd',\n",
       "   'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/',\n",
       "   'title': 'Automic Dollar Universe Consultant and Corporate Trainer',\n",
       "   'description': 'Dollar Universe, AWA Consultant, Architect, Integrator and Corporate Trainer\\n –  IBM, Wipro, TCS\\n\\n•\\tProvide scheduling product support for CA Autosys, Tidal. Consult on BMC Control-M and ORSYP Dollar Universe\\n•\\tI have administrative experience with Automic product.\\n•\\tSetup multiple UVMS cluster with DUAS active/ passive mode in dollar universe , integration with LDAP/ SSO product\\n•\\tConfigure complete workflow and pipeline to integrate with AWS cloud in automic product \\n•\\tRecently provided consulting services to migrate CA Autosys workloads to BMC Control-M for Unix and Windows at Baxter. Provided scheduling migration technique, provided requirements and migration plan, executed conversion of Autosys job files to Control-M scheduling format. Completed Proof of Concept and Phase one efforts for Autosys to Control-M migration.\\n•\\tIntegrating and managing cluster of $U and also write custom plugin as per our client use cases\\no\\tArchitecture of $Universe with parameters of $Universe and Logs management and logs retention of $Universe.\\no\\tData/Objects retrieval methods of $Universe and Performance optimization required for $Universe administration with Product patch installation.\\no\\tCoordinate with vendor in resolving product issues with Alerts integration with monitoring tools$Universe job scheduling\\no\\tAdding/modifying/updating of jobs on $Universe.\\no\\tScheduling of jobs in $universe for applications like SAP and integrate with docker, AWS\\no\\tFixing of job failures and debugging skills of job failures.\\no\\tAction ad hoc request on active job scheduling environment',\n",
       "   'location': None,\n",
       "   'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T082521Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=68d18721868144c85491fd670ba0d78a60f6c8bd19c3e6c876d085dc5bbb81a4'},\n",
       "  {'starts_at': {'day': 1, 'month': 2, 'year': 2012},\n",
       "   'ends_at': None,\n",
       "   'company': 'LinuxWorld Informatics Pvt Ltd',\n",
       "   'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/',\n",
       "   'title': 'Sr. Openstack Cloud Consultant and Corporate Trainer',\n",
       "   'description': 'Design, build, maintain and operate one of the largest open stack based cloud deployments with all the component,  Nova, Glance, Swift, Horizon, Keystone, Neutron, Cinder, Heat, Ceilometer, Trove, Sahara, Ironic, Zaqar, Manila, Designate, Barbican, Searchlight, Magnum, aodh, cloudkitty, congress, freezer, mistral, monasca-api, monasca-log-api, murano, panko, senlin, solum, tacker, vitrage, Watcher\\n\\nCapacity management, and provisioning. Maintaining and troubleshooting\\n\\nsetup and manage a multi-node Openstack environment with docker implement and provision with Devops tools\\n\\nDesigns, builds, and manages enterprise Redhat RHOSP and Helion OpenStack (HOS) Cloud environments. That involve creating capacity collection and monitoring capabilities, upgrade projects, customer consultations, operations, and design enhancements\\n\\nHave works as Openstack Consultant and also has strong knowledge of SDN, database design, networking and storage.',\n",
       "   'location': None,\n",
       "   'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T082521Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=68d18721868144c85491fd670ba0d78a60f6c8bd19c3e6c876d085dc5bbb81a4'},\n",
       "  {'starts_at': {'day': 1, 'month': 10, 'year': 2010},\n",
       "   'ends_at': None,\n",
       "   'company': 'LinuxWorld Informatics Pvt Ltd',\n",
       "   'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/',\n",
       "   'title': 'Sr. Machine Learning / Deep Learning / Data Scientist / NLP Consultant and Researcher',\n",
       "   'description': 'Consulting, Architect, Developing and Training \\n – PwC, WellsFargo, RBS, GE , Oracle , many more\\n\\n-> Expertise in the field of Artificial Intelligence, Deep Learning, and Computer Vision and having ability to solve problems such as Face Detection, Face Recognition and Object Detection using Deep Neural Network  (CNN, DNN, RNN, Convolution Networks etc.) and Optical Character Detection and Recognition (OCD & OCR)\\n\\n-> Worked in tools such as Tensorflow, Caffe/Caffe2, Keras, Theano, PyTorch etc.\\n\\n-> Build prototypes related to deep learning problems in the field of computer vision.\\n\\n-> Publications at top international conferences/ journals in fields related to computer vision/deep learning/machine learning / AI\\n\\n-> Experience on  tools, frameworks like Microsoft Azure ML, Chat Bot Framework/LUIS . IBM Watson / ConversationService, Google TensorFlow / Python for Machine Learning (e.g. scikit-learn),Open source ML libraries and tools like Apache Spark\\nHighly Worked on  Data Science, Big Data,datastructures, statistics , algorithms like Regression, Classification etc.\\n\\n-> Working knowlegde of Supervised / Unsuperivsed learning (Decision Trees, Logistic Regression, SVMs,GBM, etc)\\n\\n-> Expertise in Sentiment Analysis, Entity Extraction, Natural Language Understanding (NLU), Intent recognition\\n\\n-> Strong understanding of text pre-processing and normalization techniques, such as tokenization, POS tagging, and parsing, and how they work at a basic level and NLP toolkits as NLTK, Gensim,, Apac SpaCyhe UIMA etc.\\n\\n-> I have Hands on experience related to Datasets such as or including text, images and other logs or clickstreams. and have worked on text data cleaning, parsing and web scraping with regular expressions for text extraction and cleansing\\n\\n->Worked on recommendation algorithm used in e-commerce and advertising and  have knowledge of applied machine learning to solve real-world problems and Applied unsupervised, supervised, and semi-supervised ML in own problem domain',\n",
       "   'location': None,\n",
       "   'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T082521Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=68d18721868144c85491fd670ba0d78a60f6c8bd19c3e6c876d085dc5bbb81a4'},\n",
       "  {'starts_at': {'day': 1, 'month': 10, 'year': 2009},\n",
       "   'ends_at': None,\n",
       "   'company': 'LinuxWorld Informatics Pvt Ltd',\n",
       "   'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/',\n",
       "   'title': 'Sr. Ping Identity, Ping Federation and Ping Access Consultant and Corporate Trainer',\n",
       "   'description': 'Ping Identity, Ping Federation and Ping Access Consulting, Architect, Administration and Training \\n – IBM, Wipro, WellsFargo, many more\\n\\n• Hands on experience on implementing web SSO solution using Ping Access & Ping Federation products from PingIdentity\\n• Consult to Drive and lead IAM Security projects for the organization\\n• Subject Matter Expert in MFA / SSO / Ping Federate / SAML  / OAuth 2.0 / OpenID Connect(OIDC)\\n• Managed Corporate Ping Federate Environment for SAML partnerships and Federation SSO with third party applications\\n\\n•\\tHands on with building custom adapters using Java SDK\\n•\\tImplementing rule based web SSO for enterprise users\\n•\\tExperience with PKI, X.509 certificates,SSL/TLS\\n•\\tExperience on following: \\n           o Identity & Access Management and/or federation experience \\n           o SAML,OAuth,OpenID,SOAP, Kerberos.\\n\\nExperience in IAM  field especially in engineering.\\n•\\tInstallation and conf  of PingAccess and PingFederate in a cluster environment\\n•\\tComplex policy conf of PingAccess and PingFederate products for implementing the WAM solution\\n•\\tIdentity federation configuration experience including STS using PingFederate\\n•\\tTroubleshooting knowledge of PingAccess and PingFederate.\\n•\\tJava development experience for customization of PingAccess and PingFederate.\\n\\nReplaced/upgraded Oracle Access Manager & CA siteminder infrastructure\\n- Built out federation infrastructure with vendor product (PingIdentity PingFederate 6.x)\\n- Designed and developed an extensible web app using PingFederate technology (Open Token API); migrated over differenent different homegrown federation integrations onto the PingFederate-integrated web application solution\\n- Designed and developed an extensible single log on (SLO) authentication web portal\\n- Designed and developed a flexible and configurable identity data exchange platform in Java using LDAP\\n\\n- Standardized and implemented operational procedures: documentation, version control, monitoring, backup, support',\n",
       "   'location': None,\n",
       "   'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T082521Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=68d18721868144c85491fd670ba0d78a60f6c8bd19c3e6c876d085dc5bbb81a4'},\n",
       "  {'starts_at': {'day': 1, 'month': 2, 'year': 2008},\n",
       "   'ends_at': None,\n",
       "   'company': 'LinuxWorld Informatics Pvt Ltd',\n",
       "   'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/',\n",
       "   'title': 'Sr Devops Principal Consultant, Integrator, Architect and Corporate Trainer',\n",
       "   'description': 'Devops Principal Consultant, Architect, Integrator and Corporate Trainer\\n – Dell EMC, Wipro, TCS, WellsFargo, JP Morgan Chase, GE, CA technology, many more\\n\\n\\n•\\tIntegrate and provide consulting with large client base to devops chef tools for build design to testing tools with BDD and TDD approach like serverspec,chefspec,inspec,rspec, foodcritic,rubocop,berkshelf integrate with test kitchen with driver base of ec2, vagrant, docker\\n•\\tImplement chef using planned cookbook, roles, environment and by creating custom resource (LWRP2), library with encrypted and data bag vault\\n•\\tDeploy Ansible devops tools with ansible container and tower and write playbook for cassadra, bigdata hadoop and spark infrastructure.\\n•\\tTuned ansible play with facts and resources and create custom fact and resource module\\n•\\tBuild large infrastructure with Puppet devops tools with dynamic node classifier with ENC\\n•\\tImplement Puppet Hiera and create custom facts and modules and optimized classes and manifest file\\n•\\tHave working experience with managing Micro Services implementing using  Docker \\n•\\tImplement system deployment & management systems (Chef/Puppet/Ansible/Salt)\\n•\\tImplement complete workflow with Devops developer tools – GIT/svn/perforce/AWS codecommit, CodeHub, Jira, Artifactory/ nexus sonatype, Grunt, Gulp with Java and other build tools - Gradle, Ant, Maven, Rake\\n•\\tIntegrate CI/CD pipeline using Jenkins/Hudson/teamcity according to client needs that complete  code workflow management with AWS codepipeline / Gitlab with CI – continuous inspection server sonarqube\\n•\\tImplement workflow management with automate dollar universe\\n•\\tIntegrate Monitoring and reporting tools for e.g. Nagios, New Relic with devops automate workflow and artifact and apps business transition monitoring with APM tools appdynamics\\n•\\tImplementing  WebServers and LoadBalancers Apache HTTP Server, Apache Traffic Server, Nginx, HAProxy with Jboss/tomcat and integrate elastic search,logstach and kibaba stack (ELK)',\n",
       "   'location': None,\n",
       "   'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T082521Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=68d18721868144c85491fd670ba0d78a60f6c8bd19c3e6c876d085dc5bbb81a4'},\n",
       "  {'starts_at': {'day': 1, 'month': 10, 'year': 2007},\n",
       "   'ends_at': None,\n",
       "   'company': 'LinuxWorld Informatics Pvt Ltd',\n",
       "   'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/',\n",
       "   'title': 'Sr AWS Cloud Principal Consultant, Integrator, Architect and Corporate Trainer',\n",
       "   'description': 'AWS Cloud Principal Consultant, Architect, Integrator and Corporate Trainer\\n – Samsung R&D, Dell EMC, Wipro,JP Morgan Chase, GE, CA technology, many more\\n\\n•\\tAutomating infrastructure using AWS Opsworks for configuration mngt\\n•\\tConfigure Event driven architecture using Lambda service\\n•\\tImplement Scalable and microservices over Elastic beanstalk\\n•\\tIntegrate Redis from Elasticache with onboarding apps\\n•\\tImplement  Amazon Web Services (AWS) extensively in a large AWS cross-account environment  IAM and federated SSO  PingFederate\\n•\\tTuning and Scaling Service with alerting, monitoring optimize uptime.\\n•\\tManaged AWS services with SDK and Create Multitier stack integrated with SQS and Configuration management and automation experience with Ansible,Python Boto\\n•\\tExperience with databases such as Oracle, Mysql, Postgres, DynamoDB and Redshift\\n•\\tProduction experience with Docker and Implemented through ECS services\\n•\\tStrong Experience with All the AWS components like EC2, ELB, Auto Scaling, launch Configurations, S3, Glacier, Lifecycle rules for storage, VPC, Route 53, Cloud formation,Cloud watch, Cloud Trial,Volume,Code Pipeline, SNS, IAM and Roles\\n•\\tArchitect of public and private Workload and integrate onboarding application with Openstack and  cloud platforms (AWS), Data migration on AWS\\n•\\tRedesigned infrastructure for high availability using multiple AWS availability zones using puppet\\n•\\tImplement Complete pipeline on DevOps using  Puppet\\n•\\tSetup all AWS services with  AWS CLI & APIs and integrate with Mobile Apps \\n•\\tImplementing Splunk Cluster on AWS and Experienced in monitoring health check\\n•\\tRun Bigdata Related Apps and Workload on AWS with batch processing using EMR and real time streaming integrate with SPARK and Kinesis\\n•\\tConsulting in transforming on-premise solutions to cloud and Design solutions for Private, Hybrid and Public models and business cases can articulate on the TCO and ROI and defined architecture design methodologies such as TOGAF, Zachman',\n",
       "   'location': None,\n",
       "   'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T082521Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=68d18721868144c85491fd670ba0d78a60f6c8bd19c3e6c876d085dc5bbb81a4'},\n",
       "  {'starts_at': {'day': 1, 'month': 10, 'year': 2002},\n",
       "   'ends_at': None,\n",
       "   'company': 'LinuxWorld Informatics Pvt Ltd',\n",
       "   'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/',\n",
       "   'title': 'Chief Technical Officer (CTO)',\n",
       "   'description': 'Core responsibilities included managing scientific and technological issues within the organisation for enhanced operational efficiency, reduced process-related barriers and improved performance achievement. \\n\\nExecutive Roles: \\n\\nIdentifying the new technologies and making these vital part of organisational success\\nLeveraging on the benefits of technologies across business units\\nDriving business strategy into workable practices\\nDriving the learning modules of the company \\nDriving revenues from functional objective\\nCollaborate and communicate with client \\nReducing process-related obstacles\\n\\nOthers:\\n\\nParticipating in Linux and open source-based seminars and workshops across India \\nAs a speaker at National & International Conferences on various technologies namely Big Data Analytics, Cloud Computing, Hadoop Framework - The Need of the Day for Data Analysis, Modern programming languages namely R programming, Python, Perl and many more..\\nContributing to authority Linux and Open Source publications\\n\\nMiddleware Project :\\nRed Hat Jboss Data Virtualization to Integrate multiple data sources with diff. Modelling  and create federation with optimized performance and expose each services with RESTful API\\n\\nHave Done Altova MAPFORCE  for XML creations and generate dynamic code using map force\\nWorking with flat and other file format and mapping with database in MapForce\\n\\nGuest lectures in institutions',\n",
       "   'location': 'Jaipur Area, India',\n",
       "   'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T082521Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=68d18721868144c85491fd670ba0d78a60f6c8bd19c3e6c876d085dc5bbb81a4'}],\n",
       " 'education': [],\n",
       " 'languages': ['English', 'Hindi'],\n",
       " 'accomplishment_organisations': [],\n",
       " 'accomplishment_publications': [],\n",
       " 'accomplishment_honors_awards': [{'title': 'Invited as Judge of Grand Finale of Smart India Hackathon 2019 - Central Govt Initiative',\n",
       "   'issuer': 'India Central Govt',\n",
       "   'issued_on': {'day': 1, 'month': 3, 'year': 2019},\n",
       "   'description': 'Invited as Judge of Grand Finale of Smart India Hackathon 2019 - Central Govt Initiative\\n\\nInaugurated by Shri Prakash Javdekar (Minister - HRD India) with physical presence at Jaipur and at the same time addressed by Shri Narendra Modiji (live conference)\\n\\nWorked with Ministry of Power dignitaries...To judge the problem statement given by Government\\n'},\n",
       "  {'title': 'FIRST PERSON IN THE WORLD who has acquired the Title of REDHAT CERTIFIED ARCHITECT LEVEL 22 WITH REDHAT CERTIFIED ARCHITECT ENTERPRISE APPLICATION LEVEL 7',\n",
       "   'issuer': None,\n",
       "   'issued_on': {'day': 1, 'month': 2, 'year': 2019},\n",
       "   'description': 'I Created History - A World Record....\\n\\nA proud moment for me, PROUDEST moment for India.. I am humbled & proud to announce that I have become the FIRST PERSON IN THE WORLD who has acquired the Title of REDHAT CERTIFIED ARCHITECT LEVEL 22 WITH REDHAT CERTIFIED ARCHITECT ENTERPRISE APPLICATION LEVEL 7\\n\\nAll this is dedicated to my Country, my Students & my Family\\n\\nA giant leap towards \"Making India , Future Ready\".. The Ones who adapt with Change is successful but THE ONES WHO CAUSE THE CHANGE LEAD IN TECHNOLOGY...A message to Our Country, a message to the World, the GENIUS INSIDE YOU IS ALL SET TO COME OUT, JUST SHOW HIM /HER THE RIGHT PATH...\\n\\nAs I always quote \"I am working towards making India and we Indians as CREATOR OF TECHNOLOGIES, to do so WE SHOULD BE AHEAD FROM REST OF THE WORLD and So did I\"\\n\\nThanks everyone for your extended support'},\n",
       "  {'title': 'Awarded as \"Distinguish Alumni Award\" for distinguished achievements in his field — at JECRC Foundation',\n",
       "   'issuer': None,\n",
       "   'issued_on': None,\n",
       "   'description': None},\n",
       "  {'title': 'LinuxWorld awarded as the \"Best Internship and Maximum Growth Partner in entire Asia Pacific and India\" by RedHat Global Head Mr. Randy Russell and APAC Head Mr. Vikram Motiani @ Hotel The Taj Mahal Palace, Mumbai',\n",
       "   'issuer': 'RedHat',\n",
       "   'issued_on': None,\n",
       "   'description': 'Unbelievable 2 Awards in 1 go @ RHTPC, 2018 at Mumbai\\n\\nLinuxWorld awarded as the \"Best Internship and Maximum Growth Partner in entire Asia Pacific and India\" by RedHat Global Head Mr. Randy Russell and APAC Head Mr. Vikram Motiani @ Hotel The Taj Mahal Palace, Mumbai...\\n\\nWe won 2 awards one after the another...Entire RedHat Team was talking about our achievements we made during Summer Internship 2018...\\n\\nThe entire credit goes to our students, it wouldn\\'t have been possible without their unconditional love and support..\\n\\nThanks to RedHat and LW team'}],\n",
       " 'accomplishment_patents': [],\n",
       " 'accomplishment_courses': [],\n",
       " 'accomplishment_projects': [{'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'title': 'BigData – Data Analysis Platform',\n",
       "   'description': \"•\\tSetup Complete Big Data Hadoop Technology of Multi Node Cluster \\n•\\tManage Each and Every Component of big data for distributed storage HDFS, distributed computing MapReduce , Distributed Scheduling YARN \\n•\\tWorks on Big Data Hadoop Related Framework like PIG, HIVE, HBase, Oozie, Zookeeper, Flume , Sqoop, Splunk many more \\n•\\tDeveloped Own Web Application to implement big data hadoop infrastructure at one click with YARN Resource Manager & Managed Hadoop 2 Resource Manager Using Mesos \\n•\\tDeployed Many Processing Engine and Apps Over Spark Framework \\n•\\tWorked on Real Streaming Processing on Storm API and integrated with Kafka MQ \\n•\\tDeveloped lot of business logics on Scala Platform using Spark API \\n•\\tMachine Learning using Mahout \\n•\\tWorked and Deployed NoSQL and Cluster Based database System like HBase, Cassandra, couch-base, Dynamo DB, Maria DB to manage unstructured or Semi –structured data \\n•\\tDevelop Data Analysis and Mining Project using R & SAS programming \\n•\\tCloud Based data analysis tools – Amazon EMR \\n•\\tElasticsearch Architect and Implementation Specialist, Logstash and Kibana Implementation Specialist.\\n•\\tInstallation, configuration and maintenance of Elasticsearch Linux server based system and network applications.\\nWorked on Azure/AWS Cloud for setting up ES Cluster for API's.\\nFixing Bugs related to ES.\\nWriting ES Query for Development.\\nOptimizing the performance of Elastic Search server.\\nPreparation of ES Settings and Mapping for Elastic Search Indices.\\nBulk loading data Using Curl commands.\\nUpgrading & Maintenance of ES cluster.\\nWorked in shell scripts for automatic bulk load.\\n• Built high-throughput and fault-tolerant river to pre-compute and pipe MongoDB, and RabbitMQ data into Elasticsearch\\n• Built a tool which simplies the creation of complex nested Elasticsearch queries\\n• Transformed data from Apache Spark into a schema optimized for complex analytics in Elasticsearch\",\n",
       "   'url': None},\n",
       "  {'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'title': 'Cloud ComputingTechnologies – Deployment, Testing & Security',\n",
       "   'description': '•\\tManaging and Configuring Virtualization Technologies Over the Hypervisor Xen, KVM, EXSi \\n•\\tWorks and and Managed Full RedHat Virtualization Technology Product RHEVM, with complete migration and datacenter Configuration \\n•\\tWorks on Virtualization Products Like VirtualBox, VMware Workstation, Virt-Manager \\n•\\tDevelop Over own Virtualization Frontend Over KVM using Python API \\n•\\tFully Configure and Manage Amazon AWS Cloud for Huge Topology \\n•\\tSet Each and Every Amazon AWS Products like EC2, VPC, ELB, S3, Glacier, EMR, DynamoDB, EBS,RDS, IAM, many more \\n•\\tWorks at Architect Level on OpenStack Cloud and Configure for Public and Private Architect \\n•\\tManaging and Implementing Each Stack of Openstack like Nova, Cinder, Glance, Keystone, Horizon, Qpid, Swift, Neutron/Quantum \\n•\\tSetup Multi Node Environment of OpenStack Over VLAN, GRE tunnel \\n•\\tConfigure Distributed Cloud Storage using GlusterFS filesystem \\n•\\tDeploy our OWN cloud and Architect each Service by own code using Python like Saas, Staas, Naas, Paas, Iaas \\n•\\tCloud PaaS – RedHat OpenShift & Google App Engine \\n•\\tCloud IaaS – RedHat OpenStack, AWS, Microsoft Azure & Google Compute Engine \\n•\\tManaging and Configuring Message Queue in distributed application using Apache Qpid, RabbitMQ, Apache AMQ, Amazon SQS \\n•\\tSetup Advanced Message Queuing Protocol (AMQP) for Middleware interoperability \\n',\n",
       "   'url': None},\n",
       "  {'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'title': 'DevOps & Containers Automation Technologies',\n",
       "   'description': '•\\tManage and Implement Project Build Tools using Apache Maven & Nexus Sonatype Repository Manager \\n•\\tManaging DevOps Platform using Chef, Ansible, CF Engine and Puppet for building Continuous automated delivery services \\n•\\tExperience in Building and Troubleshooting Continuous Integration CI using Hudson/ Jenkins \\n•\\tWork on Revision and Source Control System Perforce / GIT / SVN \\n•\\tWorked on Containerized Technologies viz Dockers, Rocket, Kubernetes, Swarm, Magnum, Amazon Container Service, RedHat Atomic Host & Core OS. \\n',\n",
       "   'url': None},\n",
       "  {'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'title': 'Programming Languages',\n",
       "   'description': '•\\tHas Done High-End Automation scripting in Bash, Perl & Ruby \\n•\\tWorked on other languages namely Python, PHP, Java Script, Scala, Go\\n',\n",
       "   'url': None},\n",
       "  {'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'title': 'RedHat & Linux Based Technologies',\n",
       "   'description': '•\\tWrite SELinux custom policy for enterprise level application with their daemon type transition, port binding, boolean to control security feature \\n•\\tManage and configure MLS and MCS using SELinux for high security driven platform for enterprise \\n•\\tWorked on docker container MCS separation by svirt and enabled customize enforcing type of different container \\n•\\tManaging Linux System Administrator Task like Performance Tuning, Kernel Re- compiling, MBR and GPT parameter tuning, Format understaning like ext2/ext3/ext3/xfs, Partition managing like swap, raid,lvm, runlevel till rhel6 version and Systemd management from RHEL7,many more \\n•\\tConfiguring Linux Server on RHEL server like telnet, openssh, ftp/sftp/ftps, http/https, samba, nfs, dns,dhcp, nis,smtp sendmail, smtp postfix, ldap,ntp, tftp, many more \\n•\\tImplement and Test Security on Linux System like SELinux, Firewall – Iptables / Firewalld, ACL, Implement Kerberos Server and AAA server – TACACS/ Radius \\n',\n",
       "   'url': None},\n",
       "  {'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'title': 'Security Implement & Pen Security - Web Application Technologies – Development',\n",
       "   'description': \"•\\tImplement Ping Identity's PingFederate Single Sign On (SSO) solution for centralized management and audit of user program access. Users simply log on to a PingFederate portal, then access a pre-selected list of applications without need to re-authenticate.\\nPingFederate, Security, Oauth 2.0, e-business security, SAML, Social Login, Identity providers - Google, Facebook, Digidentity, SSO, Expert opinions, Security scans, Federated Idenity & Access management, Cloud Identity\\n•\\tAudit and Testing Web Bugs and Analyse Security Vulnerability like SQL-Injection, XSS, CSRF, Cookie Manipulation, Buffer Overflow, Many more in Web Application. \\n•\\tPenetration Testing Risk Management Network Security Code Audits, Malware Analysis \\n•\\tVulnerability Assessment Business Continuity Forensic Investigation Exploit Development \\n•\\tSpecialized experience in Information Assurance and security program management, risk management, network and systems penetration testing and vulnerability assessment \\n•\\tProficient in the testing and analysis of benchmarking the application against industry standards such as OWASP ASVS, Top 10 and SAN \\n•\\tExperience on IDS/IPS like Snort, OSSEC and implementation of VPN tunnels over IPSec. Experience in implementing and managing IPSec tunnels across different production network \\n•\\tCisco SourceFire Managed Devices - Using FireSIGHT Defence Center, NGIPS Sensors & ASA FirePOWER\\n•\\tApplication security assessment of numerous web applications and mobile applications using Professional Burp Suite, Acunetix,etc. \\n•\\tIntegrating Web services Architecture API using SOAP and REST to parse web data \\n•\\tConfigure Web Server Like Apache Tomcat, Apache HTTPD, nginx with SSL, Proxy, Authentication Support and Tuned or Optimized for Performance and Load Analysis \\n•\\tImplement Infrastructure Monitoring tools like Nagios and Tuned Some internal SNMP Feature for Extra Support and Connect to External API for Reporting \\n\\n\",\n",
       "   'url': None}],\n",
       " 'accomplishment_test_scores': [],\n",
       " 'volunteer_work': [],\n",
       " 'certifications': [{'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'name': 'AWS Certified Big Data - Specialty',\n",
       "   'license_number': '3KZFXFBCGJEE18GR',\n",
       "   'display_source': None,\n",
       "   'authority': 'Amazon Web Services (AWS)',\n",
       "   'url': None},\n",
       "  {'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'name': 'AWS Certified Machine Learning – Specialty',\n",
       "   'license_number': '1489E5BKMM4EQEGG',\n",
       "   'display_source': None,\n",
       "   'authority': 'Amazon Web Services (AWS)',\n",
       "   'url': None},\n",
       "  {'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'name': 'AWS Certified Solutions Architect - Associate 2019',\n",
       "   'license_number': '0STNT1T1EBQQ1G3S',\n",
       "   'display_source': None,\n",
       "   'authority': 'Amazon Web Services (AWS)',\n",
       "   'url': None},\n",
       "  {'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'name': 'Amazon Web Services Certified Developer Associate',\n",
       "   'license_number': None,\n",
       "   'display_source': None,\n",
       "   'authority': 'Amazon Web Services (AWS)',\n",
       "   'url': None},\n",
       "  {'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'name': 'Amazon Web Services Certified Solutions Associate',\n",
       "   'license_number': None,\n",
       "   'display_source': None,\n",
       "   'authority': 'Amazon Web Services (AWS)',\n",
       "   'url': None},\n",
       "  {'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'name': 'Amazon Web Services Certified SysOps Associate',\n",
       "   'license_number': None,\n",
       "   'display_source': None,\n",
       "   'authority': 'Amazon Web Services (AWS)',\n",
       "   'url': None},\n",
       "  {'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'name': 'Certified Blockchain Expert',\n",
       "   'license_number': None,\n",
       "   'display_source': None,\n",
       "   'authority': 'Blockchain Council',\n",
       "   'url': None},\n",
       "  {'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'name': 'Certified Information Systems Security Professional (CISSP)',\n",
       "   'license_number': None,\n",
       "   'display_source': None,\n",
       "   'authority': None,\n",
       "   'url': None},\n",
       "  {'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'name': 'Chef DevOps Cookbook Development Badge Certification',\n",
       "   'license_number': None,\n",
       "   'display_source': None,\n",
       "   'authority': 'Progress Chef',\n",
       "   'url': None},\n",
       "  {'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'name': 'Cisco Certified Internetwork Expert Routing n Switching (CCIE R&S) *',\n",
       "   'license_number': None,\n",
       "   'display_source': None,\n",
       "   'authority': 'Cisco',\n",
       "   'url': None},\n",
       "  {'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'name': 'Cisco Certified Internetwork Expert Security (CCIE Security) *',\n",
       "   'license_number': None,\n",
       "   'display_source': None,\n",
       "   'authority': 'Cisco',\n",
       "   'url': None},\n",
       "  {'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'name': 'Cisco Certified Network Associate (CCNA - R&S and Security)',\n",
       "   'license_number': None,\n",
       "   'display_source': None,\n",
       "   'authority': 'Cisco',\n",
       "   'url': None},\n",
       "  {'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'name': 'Cisco Certified Network Professional (CCNP - R&S and Security)',\n",
       "   'license_number': None,\n",
       "   'display_source': None,\n",
       "   'authority': 'Cisco',\n",
       "   'url': None},\n",
       "  {'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'name': 'Cisco Certified System Instructor (CCSI)',\n",
       "   'license_number': None,\n",
       "   'display_source': None,\n",
       "   'authority': 'Cisco',\n",
       "   'url': None},\n",
       "  {'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'name': 'Cloudera Certified Administrator for Apache Hadoop (CCAH)',\n",
       "   'license_number': None,\n",
       "   'display_source': None,\n",
       "   'authority': 'Cloudera',\n",
       "   'url': None},\n",
       "  {'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'name': 'Computer Hacking Forensic Investigator (CHFI)',\n",
       "   'license_number': None,\n",
       "   'display_source': None,\n",
       "   'authority': 'EC-Council',\n",
       "   'url': None},\n",
       "  {'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'name': 'EC- Council Certified Ethical Hacker - V7',\n",
       "   'license_number': None,\n",
       "   'display_source': None,\n",
       "   'authority': None,\n",
       "   'url': None},\n",
       "  {'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'name': 'EC-Council Certified Instructor',\n",
       "   'license_number': None,\n",
       "   'display_source': None,\n",
       "   'authority': None,\n",
       "   'url': None},\n",
       "  {'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'name': 'EC-Council Certified Security Analyst (ECSA)',\n",
       "   'license_number': None,\n",
       "   'display_source': None,\n",
       "   'authority': None,\n",
       "   'url': None},\n",
       "  {'starts_at': None,\n",
       "   'ends_at': None,\n",
       "   'name': 'Google cloud Platform Associate Cloud Engineer',\n",
       "   'license_number': None,\n",
       "   'display_source': None,\n",
       "   'authority': 'Google',\n",
       "   'url': None}],\n",
       " 'connections': None,\n",
       " 'people_also_viewed': [{'link': 'https://www.linkedin.com/in/rahul-cse',\n",
       "   'name': 'Rahul Maheshwari',\n",
       "   'summary': 'Er-Adobe, Ex-Google | Founder - LinuxSocials | Philanthropist | Speaker',\n",
       "   'location': None},\n",
       "  {'link': 'https://www.linkedin.com/in/preeti-chandak',\n",
       "   'name': 'Preeti Chandak',\n",
       "   'summary': 'Chief Strategy Officer at LinuxWorld Informatics Pvt Ltd | Training Co-Ordinator (Corporate & Retail -IT) | Entrepreneur',\n",
       "   'location': None},\n",
       "  {'link': 'https://www.linkedin.com/in/shrishti-kapoor',\n",
       "   'name': 'Shrishti Kapoor',\n",
       "   'summary': 'FTE @Cognizant | 2 x RedHat Certified | Aviatrix Certified Engineer | AI-900 | Technical Trainer | IIEC-DOT Volunteer | Technical Research Writer Enthusiast..!! 🔷DevOps 🔷Cloud-AWS, Azure 🔷ML/AI/DL',\n",
       "   'location': None},\n",
       "  {'link': 'https://www.linkedin.com/in/priyansh-magotra-a98282141',\n",
       "   'name': 'Priyansh Magotra',\n",
       "   'summary': 'TSE@Red Hat | x6 Red Hat Certified | OpenShift Support Delivery | Kubernetes | DevOps (Docker/Podman/CRI-O/Buildah | Ansible | Git | Jenkins) | RedHat Linux',\n",
       "   'location': None},\n",
       "  {'link': 'https://www.linkedin.com/in/saiyampathak',\n",
       "   'name': 'Saiyam Pathak',\n",
       "   'summary': 'Director of Technical Evangelism at Civo | CNCF Ambassador | Author | Founder @kubesimplify',\n",
       "   'location': None},\n",
       "  {'link': 'https://www.linkedin.com/in/rohit-ghumare',\n",
       "   'name': 'Rohit Ghumare',\n",
       "   'summary': 'DevRel🥑 @solo.io | Founder @Keep Up(20K+)💙 | R&D🔍 | Public Speaker🎙️| Community Builder @AWS☁️ | DevOps, Data Science, MLOps | Technical Teaching Assistant | Kubernetes, Istio & Rasa Official Contributor',\n",
       "   'location': None},\n",
       "  {'link': 'https://www.linkedin.com/in/raktimmidya',\n",
       "   'name': 'Raktim Midya',\n",
       "   'summary': 'Aspiring Freelancer & DevOps Consultant ★ Taught Hundreds of Students & Still Teaching ★1x AWS, 5x RedHat, 4x Microsoft Certified ★ AWS Community Builder ★ Technical Content Creator @Medium @YouTube',\n",
       "   'location': None},\n",
       "  {'link': 'https://www.linkedin.com/in/dipadityadas',\n",
       "   'name': 'Dipaditya Das',\n",
       "   'summary': 'Aspiring Software Development Engineer || 4x Red Hat Certified || 7x Microsoft Certified || AWS Community Builders || Google Cloud Facilitator || Technical Volunteer at ARTH || Open Source Contributor',\n",
       "   'location': None},\n",
       "  {'link': 'https://www.linkedin.com/in/sonikaushal',\n",
       "   'name': 'Kaushal Soni',\n",
       "   'summary': '🔔 Youtuber @DevopsGyan ➤ 2xRedHat Certified ➤ GCP Certified ➤ Cloud Engineer @ Searce ➤ DevOps & AWS Instructor ➤ Experience in DevOps (🐳☸👩🏻\\u200d🍳), AWS☁️ & ML 🧠 \\xa0Stack',\n",
       "   'location': None},\n",
       "  {'link': 'https://www.linkedin.com/in/20ashisharma',\n",
       "   'name': 'Ashi Sharma',\n",
       "   'summary': 'I want to be CREATOR of Technology | Learn to Lead | DevOps | Cloud | Arthlearner | Ansible | Docker | Kubernetes | Jenkins | AWS | Linux | ML',\n",
       "   'location': None}],\n",
       " 'recommendations': [],\n",
       " 'activities': [],\n",
       " 'similarly_named_profiles': [{'name': 'Vimal Daga',\n",
       "   'link': 'https://in.linkedin.com/in/vimal-daga-9219931ba',\n",
       "   'summary': '--',\n",
       "   'location': 'Sehore'},\n",
       "  {'name': 'vimal daga',\n",
       "   'link': 'https://in.linkedin.com/in/vimal-daga-9288a232',\n",
       "   'summary': 'Owner of Rishabh international star trading house',\n",
       "   'location': 'Jodhpur'},\n",
       "  {'name': 'Vimal Daga',\n",
       "   'link': 'https://in.linkedin.com/in/vimal-daga-5467a8108',\n",
       "   'summary': 'CEO at SSJ',\n",
       "   'location': 'Mumbai'},\n",
       "  {'name': 'Vimal Daga',\n",
       "   'link': 'https://in.linkedin.com/in/vimal-daga-46847a9b',\n",
       "   'summary': 'Food Production Professional',\n",
       "   'location': 'Raipur'}],\n",
       " 'articles': [],\n",
       " 'groups': [],\n",
       " 'phone_numbers': [],\n",
       " 'social_networking_services': [{'service': 'github',\n",
       "   'canonical_url': 'github.com/vimallinuxworld13',\n",
       "   'internal_id': None}],\n",
       " 'skills': ['Linux',\n",
       "  'Cloud Computing',\n",
       "  'Big Data',\n",
       "  'OpenStack',\n",
       "  'Dev Ops',\n",
       "  'Splunk',\n",
       "  'Ping Federate',\n",
       "  'Docker',\n",
       "  'Ansible',\n",
       "  'Chef',\n",
       "  'Big Data Analytics',\n",
       "  'Storage - Glusterfs',\n",
       "  'Apache Hadoop',\n",
       "  'R Programming',\n",
       "  'Amazon Web Services (AWS)',\n",
       "  'Enterprise Network Security',\n",
       "  'Web Application Security',\n",
       "  'Web Application Security Assessment',\n",
       "  'Perl',\n",
       "  'Cisco Technologies'],\n",
       " 'inferred_salary': {'min': None, 'max': None},\n",
       " 'gender': None,\n",
       " 'birth_date': None,\n",
       " 'industry': None,\n",
       " 'extra': {'github_profile_id': None,\n",
       "  'twitter_profile_id': None,\n",
       "  'facebook_profile_id': None},\n",
       " 'interests': [],\n",
       " 'personal_emails': ['vdaga@lwindia.com'],\n",
       " 'personal_numbers': []}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a59150b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b3e24d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2c11966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42c1c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77d47989",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model='gpt-3.5-turbo',  \n",
    "    temperature=1, \n",
    "    openai_api_key=my_openai_key \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7b2b2f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "54debbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytemplate = \"\"\"Given is the information  about someone linkedin profile: \n",
    "  {someoneinformation} :  what u have to do, list given below :\n",
    "        1. tell me summary about in 1 lines\n",
    "        2. 2 unique point about it\n",
    "        3. tell me about it in 5 words\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f966a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "myprompttemplate = PromptTemplate(\n",
    "                        input_variables=[\"someoneinformation\"] , \n",
    "                        template=mytemplate \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f9bf7abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mychain = LLMChain(\n",
    "        llm=model , \n",
    "        prompt=myprompttemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "49b7cb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Summary: Vimal Daga is a technologist, motivational speaker, IT consultant, and corporate trainer with expertise in cutting-edge technologies such as machine learning, artificial intelligence, and cloud computing. He provides training and consultancy to top-level management and has worked with various successful clients.\n",
      "\n",
      "2. Two unique points:\n",
      "- Vimal Daga emphasizes that no technology is too challenging or complex if approached with the right mindset and understanding.\n",
      "- He provides consultancy services specifically aimed at helping companies decide on the right technology to adopt, enhancing their technical skills, and managing complex project environments.\n",
      "\n",
      "3. Five words to describe Vimal Daga's profile: Technologist, Trainer, Consultant, Motivational Speaker\n"
     ]
    }
   ],
   "source": [
    "print( mychain.run(someoneinformation=data['summary']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "347ec7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create final prompt\n",
    "#myprompttemplate.format(things=\"food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9f8d7a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# myinfo = myprompttemplate.format(someoneinformation=data['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "91ea9cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7330857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2aaca208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e20294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "be088e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "14500a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "yourCustomTool = [ \n",
    "    \n",
    "    \n",
    "    Tool( \n",
    "    name=\"Crawl Vimal daga linkedin profile page\",\n",
    "    func=lw_scrape_linkedin,\n",
    "    description=\"useful for when you need get the Linkedin Page URL\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "275aef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yourCustomTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "860bd317",
   "metadata": {},
   "outputs": [],
   "source": [
    "myagent = initialize_agent(\n",
    "    llm=model,  \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    tools=yourCustomTool\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9143db44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI can extract the information about Vimal Daga's LinkedIn profile from the given text.\n",
      "Action: Crawl Vimal daga linkedin profile page\n",
      "Action Input: Vimal Daga\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'public_identifier': 'vimaldaga', 'profile_pic_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/person/vimaldaga/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T104855Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0bbf3fe4d8fb7a61050209f9b78f264f7adb69533c4dacdc50d1dd2459ebfa7b', 'background_cover_image_url': None, 'first_name': 'Vimal', 'last_name': 'Daga', 'full_name': 'Vimal Daga', 'follower_count': None, 'occupation': 'Sr. Machine Learning / Deep Learning / Data Scientist / NLP Consultant and Researcher at LinuxWorld Informatics Pvt Ltd', 'headline': 'World Record Holder | TEDx Speaker | Philanthropist | Sr. Principal Consultant | Entrepreneur | Founder LW Informatics', 'summary': \"Aloha, I am Vimal Daga, known as an Technologist & also a Technology Motivational Speaker, Sr. IT Consultant & Corporate Trainer having Expertize in Latest and High-End Technologies like Machine Learning, Artificial Intelligence, Deep Learning,IoT, NLP, Splunk, PingFederate, Delphix, AppDynamics, Docker, DevOps, AWS, Cloud Computing, Big Data Analytics Dollar Universe..\\n\\nEmail: vdaga@lwindia.com\\n\\nI am an IT Enthusiast, who is passionate about exploring all the latest technologies from research perspective. I inspire, train and provide Consulting Services to all the companies willing to work, migrate or use high end IT Technologies and also help MNC's to find the right approach to get best ROI.\\n◆ MY BELIEF\\nIn my own words – “No technology is challenging or difficult or complex as the world says to us or approaches to us since it is MAN-MADE. You just need the right path or approach to understand the technology and take it further as per one’s requirement”. Right thinking or positive thinking that “Yes I Can Do It” can change one’s life drastically.\\n\\n◆ MY WHAT\\nI train and provide consultancy to all the Top Level Management including the Directors, CTO’s, Project Leads and also Senior IT executives working on technologies or willing to understand whether the new technology which they are willing to adapt is the right one. It helps them in enhancing their technical skills and also ability to handle complex project based environment.\\n\\n◆ MY TECHNOLOGIES\\n•      Machine Learning, AI, DL, NLP\\n•\\tDevOps – Chef, Puppet, Ansible, Jenkins, Docker\\n•\\tOps Intelligence & Other Tools – Splunk, PingFederate, Delphix, AppDynamics\\n•\\tCloud Computing – AWS, OpenStack, \\n•\\tBigData Analytics – Hadoop, Spark, Mahout, Cassandra\\n•\\tRedHat, Cisco, Security and Many more\\n\\n◆ MY SUCCESSFUL CLIENTS\\nWells Fargo, JP Morgan Chase, Bank of America, GE, CA Technologies, Dell, TCS, Deloitte, PwC, HP, Wipro, IBM, Samsung R&D, Ericsson and many more…\\n\\nLET'S CONNECT\\nI am easy to reach M-F at 0091.982.912.5960\", 'country': 'IN', 'country_full_name': 'India', 'city': 'Bengaluru', 'state': 'Karnataka', 'experiences': [{'starts_at': {'day': 1, 'month': 10, 'year': 2009}, 'ends_at': None, 'company': 'LinuxWorld Informatics Pvt. Ltd.', 'company_linkedin_profile_url': None, 'title': 'Sr. Splunk Consultant and Corporate Trainer', 'description': 'Splunk Consulting, Architect, SIEM, Administration, Developing and Training \\n – PwC, WellsFargo, JP Morgan Chase, Deloitte,  many more\\n\\nMore Than 4 years experience in Splunk Entreprise. Deploying large data analytics architectures for customers mainly in the Banking section, Governments and Security companies. Consult and Solutions to Over 30 Splunk projects and over 40 Corporate Batch Trained\\n\\nIntegrating Splunk with Arcsight and Splunk for Security and Risk Analysis including Information Asset Management and SIEM.\\n\\n•       Architecture design and implementation of Splunk solution\\n•       Platform optimization\\n•       Design and implement indexers and search heads Clusters (+migrations)\\n•\\tImplementing distributed Splunk and ES\\n•\\tConfiguring Splunk heavy forwarders to receive logs using push or pull\\n•\\tWriting complex correlation searches for data analysis and notable events\\n•\\tWriting optimized searches for realtime and historical data analysis\\n•\\tWriting custom apps for field extractions and data normalization\\n•\\tDeveloping and deploying customized configuration apps for distributed Splunk\\n•\\tWriting field extractions for desired fields out of proprietary application data\\n•\\tCreating reports and correlated alerts for stake holders about application behavior and anomalies\\n•\\tData aggregation and summarization by summary indexing and acceleration \\n•\\tWriting transaction searches and data normalization\\n•\\tIntegrating Splunk with AD servers for ldap authentications or SAML PING FEDERATE, DBs for enrichments, Hadoop servers for parallel processing using Hadoop Connect\\n•\\tUpgrading stand-alone Splunk to distributed Splunk implementation\\n•\\tWriting custom inputs, props and transforms to have correct event-breaks, field extractions and transformations\\n•\\tStrategizing and writing custom configurations for data routing and selective indexing for the location wise forwarders\\n•\\tDeveloping interactive dashboards, reports and optimized search queries for users', 'location': None, 'logo_url': None}, {'starts_at': {'day': 1, 'month': 4, 'year': 2014}, 'ends_at': None, 'company': 'LinuxWorld Informatics Pvt Ltd', 'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/', 'title': 'Microservices Architect and Corporate Trainer', 'description': '•\\tMicroservices Spring boot/Spring cloud, Dropwizard\\n•\\tExpertise in Java/ J2EE, Microservices , Spring Boot, Spring Cloud, EJB, terms of designing and implementing systems.\\n•\\tmodel and build scalable Microservices components/ APIs \\n•\\tmodel object for no sql DB \\n•\\tmodel and build integration flows on various API development platform \\n•\\tperform pair programming \\n•\\tbuild test scripts for Microservices components \\n•\\tDecompose an existing monolithic into microservices archtecture \\n•\\tcreated microservices architecture for new development \\n•\\tDefine, develop and manage API lifecycle or PaaS \\n•\\tDefine, develop components compliant wth Microservices architecture patterns \\n•\\tDefine and model cloud based persistence storage \\n•\\tIntegrate with cloud based storage, on-prem applcations / integration layer \\n•\\tDefine, develop runtime models for Microservices architecture \\n•\\tDefine, develop highly scalable deployment model for Microservices\\n•\\tImplement microservices on Docker Containerized environment', 'location': None, 'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T104859Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=f5c09b01354219061deec91194a7b567fc778b2de37df860c76a821834f0ca7b'}, {'starts_at': {'day': 1, 'month': 2, 'year': 2014}, 'ends_at': None, 'company': 'LinuxWorld Informatics Pvt Ltd', 'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/', 'title': 'Appdynamics Consultant and Trainer', 'description': 'AWS Cloud Principal Consultant, Architect, Integrator and Corporate Trainer\\n– WellsFargo, JP Morgan Chase  many more\\n\\n•\\tManaging and Monitoring various metrics to analyze performance of application hosted on PaaS platform using Appdynamics for microservices\\n•\\tMonitor multi-tier architecture using AppDynamics Platform deployed on Cloud\\n•\\tMonitoring slow queries and finding bottlenecks\\n•\\tInstall App Agent & Machine agent\\n•\\tComplex Application monitoring in Appdynamics \\n•\\tCreate Application Dashboard and manage Transaction Snapshot / Scorecard and Diagnostic session\\n•\\tCodelevel issue repot and Monitor and Analyze\\n•\\tConfiguring Dashboards and Charting Data and Alert Log / Troubleshooting\\n•\\tCreating and Managing Reports and Portal Administration\\n•\\tAccount Details and User Management in appdynamics', 'location': None, 'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T104859Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=f5c09b01354219061deec91194a7b567fc778b2de37df860c76a821834f0ca7b'}, {'starts_at': {'day': 1, 'month': 10, 'year': 2013}, 'ends_at': None, 'company': 'LinuxWorld Informatics Pvt Ltd', 'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/', 'title': 'Sr. Docker CI/CD Architect, Consultant and Corporate Trainer', 'description': 'Dcoker Container Consulting, Architect, Administration and Training \\n – Samsung R&D, Dell EMC, Verizon, NTT,  GE, CA Technology, Oracle, IBM, Amazon, Wipro, TCS, many more\\n\\nTraining and Consulting to Team Leads, CoE, Sr. Engineers for container related technologies like Docker with Cluster and Orchestration Engine Swarm, Kubernetes, Mesosphere and entire CI/CD cycle with containers\\nTrained Cloud DevOps Engineer to raise the bar for a high-performing team that delivers large scale solutions on the cloud. Fast-moving organization to transform a large enterprise onto the next generation of Cloud.\\n\\nAutomating large scale deployments with modern configuration and deployment management systems on AWS using Docker Containers Environment with ECS and managed through Code Pipeline.\\nDelivered container based deployments using Docker and can offer solutions to the complexities of Orchestration and will drive the company towards delivering containers in production.\\n\\n•\\tExtensive experience in Continuous Integration tool Bamboo/Jenkins/Circle CI with Integration with Docker\\n•\\tExperience in automation of the deployment and configuration of the virtualized infrastructure and the entire software stack and network stack through Docker\\n•\\tExtensive experience in build configurations using Build Management tool Docker for applications in various technologies including Python, Ruby, Scala, Java and many more\\n•\\tWorked and Trained on Docker Compose to auto High-End build pipeline\\n\\nExpert on Ansible/ Chef / Puppet to provisioning Docker task and in working with scripting Languages to configuring and maintaining CI/CD environment and automate AGILE Projects', 'location': None, 'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T104859Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=f5c09b01354219061deec91194a7b567fc778b2de37df860c76a821834f0ca7b'}, {'starts_at': {'day': 1, 'month': 2, 'year': 2013}, 'ends_at': None, 'company': 'LinuxWorld Informatics Pvt Ltd', 'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/', 'title': 'Centrify IDaaS Consultant, Integrator and Corporate Trainer', 'description': 'Centrify Principal Consultant, Architect, Integrator and Corporate Trainer\\n–  IBM and many more\\n\\nIntegrated and Implement  Centrify identity management, security, SaaS single sign-on, Identity as a Service (IDaaS), mobile device management, mobile authentication, privileged access management, multi-factor authentication (MFA), privileged identity management, and identity and access management\\n\\nImplement POC of Centrify Server Suite with multiple vendor with different use cases', 'location': None, 'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T104859Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=f5c09b01354219061deec91194a7b567fc778b2de37df860c76a821834f0ca7b'}, {'starts_at': {'day': 1, 'month': 3, 'year': 2012}, 'ends_at': None, 'company': 'LinuxWorld Informatics Pvt Ltd', 'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/', 'title': 'Automic Dollar Universe Consultant and Corporate Trainer', 'description': 'Dollar Universe, AWA Consultant, Architect, Integrator and Corporate Trainer\\n –  IBM, Wipro, TCS\\n\\n•\\tProvide scheduling product support for CA Autosys, Tidal. Consult on BMC Control-M and ORSYP Dollar Universe\\n•\\tI have administrative experience with Automic product.\\n•\\tSetup multiple UVMS cluster with DUAS active/ passive mode in dollar universe , integration with LDAP/ SSO product\\n•\\tConfigure complete workflow and pipeline to integrate with AWS cloud in automic product \\n•\\tRecently provided consulting services to migrate CA Autosys workloads to BMC Control-M for Unix and Windows at Baxter. Provided scheduling migration technique, provided requirements and migration plan, executed conversion of Autosys job files to Control-M scheduling format. Completed Proof of Concept and Phase one efforts for Autosys to Control-M migration.\\n•\\tIntegrating and managing cluster of $U and also write custom plugin as per our client use cases\\no\\tArchitecture of $Universe with parameters of $Universe and Logs management and logs retention of $Universe.\\no\\tData/Objects retrieval methods of $Universe and Performance optimization required for $Universe administration with Product patch installation.\\no\\tCoordinate with vendor in resolving product issues with Alerts integration with monitoring tools$Universe job scheduling\\no\\tAdding/modifying/updating of jobs on $Universe.\\no\\tScheduling of jobs in $universe for applications like SAP and integrate with docker, AWS\\no\\tFixing of job failures and debugging skills of job failures.\\no\\tAction ad hoc request on active job scheduling environment', 'location': None, 'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T104859Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=f5c09b01354219061deec91194a7b567fc778b2de37df860c76a821834f0ca7b'}, {'starts_at': {'day': 1, 'month': 2, 'year': 2012}, 'ends_at': None, 'company': 'LinuxWorld Informatics Pvt Ltd', 'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/', 'title': 'Sr. Openstack Cloud Consultant and Corporate Trainer', 'description': 'Design, build, maintain and operate one of the largest open stack based cloud deployments with all the component,  Nova, Glance, Swift, Horizon, Keystone, Neutron, Cinder, Heat, Ceilometer, Trove, Sahara, Ironic, Zaqar, Manila, Designate, Barbican, Searchlight, Magnum, aodh, cloudkitty, congress, freezer, mistral, monasca-api, monasca-log-api, murano, panko, senlin, solum, tacker, vitrage, Watcher\\n\\nCapacity management, and provisioning. Maintaining and troubleshooting\\n\\nsetup and manage a multi-node Openstack environment with docker implement and provision with Devops tools\\n\\nDesigns, builds, and manages enterprise Redhat RHOSP and Helion OpenStack (HOS) Cloud environments. That involve creating capacity collection and monitoring capabilities, upgrade projects, customer consultations, operations, and design enhancements\\n\\nHave works as Openstack Consultant and also has strong knowledge of SDN, database design, networking and storage.', 'location': None, 'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T104859Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=f5c09b01354219061deec91194a7b567fc778b2de37df860c76a821834f0ca7b'}, {'starts_at': {'day': 1, 'month': 10, 'year': 2010}, 'ends_at': None, 'company': 'LinuxWorld Informatics Pvt Ltd', 'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/', 'title': 'Sr. Machine Learning / Deep Learning / Data Scientist / NLP Consultant and Researcher', 'description': 'Consulting, Architect, Developing and Training \\n – PwC, WellsFargo, RBS, GE , Oracle , many more\\n\\n-> Expertise in the field of Artificial Intelligence, Deep Learning, and Computer Vision and having ability to solve problems such as Face Detection, Face Recognition and Object Detection using Deep Neural Network  (CNN, DNN, RNN, Convolution Networks etc.) and Optical Character Detection and Recognition (OCD & OCR)\\n\\n-> Worked in tools such as Tensorflow, Caffe/Caffe2, Keras, Theano, PyTorch etc.\\n\\n-> Build prototypes related to deep learning problems in the field of computer vision.\\n\\n-> Publications at top international conferences/ journals in fields related to computer vision/deep learning/machine learning / AI\\n\\n-> Experience on  tools, frameworks like Microsoft Azure ML, Chat Bot Framework/LUIS . IBM Watson / ConversationService, Google TensorFlow / Python for Machine Learning (e.g. scikit-learn),Open source ML libraries and tools like Apache Spark\\nHighly Worked on  Data Science, Big Data,datastructures, statistics , algorithms like Regression, Classification etc.\\n\\n-> Working knowlegde of Supervised / Unsuperivsed learning (Decision Trees, Logistic Regression, SVMs,GBM, etc)\\n\\n-> Expertise in Sentiment Analysis, Entity Extraction, Natural Language Understanding (NLU), Intent recognition\\n\\n-> Strong understanding of text pre-processing and normalization techniques, such as tokenization, POS tagging, and parsing, and how they work at a basic level and NLP toolkits as NLTK, Gensim,, Apac SpaCyhe UIMA etc.\\n\\n-> I have Hands on experience related to Datasets such as or including text, images and other logs or clickstreams. and have worked on text data cleaning, parsing and web scraping with regular expressions for text extraction and cleansing\\n\\n->Worked on recommendation algorithm used in e-commerce and advertising and  have knowledge of applied machine learning to solve real-world problems and Applied unsupervised, supervised, and semi-supervised ML in own problem domain', 'location': None, 'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T104859Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=f5c09b01354219061deec91194a7b567fc778b2de37df860c76a821834f0ca7b'}, {'starts_at': {'day': 1, 'month': 10, 'year': 2009}, 'ends_at': None, 'company': 'LinuxWorld Informatics Pvt Ltd', 'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/', 'title': 'Sr. Ping Identity, Ping Federation and Ping Access Consultant and Corporate Trainer', 'description': 'Ping Identity, Ping Federation and Ping Access Consulting, Architect, Administration and Training \\n – IBM, Wipro, WellsFargo, many more\\n\\n• Hands on experience on implementing web SSO solution using Ping Access & Ping Federation products from PingIdentity\\n• Consult to Drive and lead IAM Security projects for the organization\\n• Subject Matter Expert in MFA / SSO / Ping Federate / SAML  / OAuth 2.0 / OpenID Connect(OIDC)\\n• Managed Corporate Ping Federate Environment for SAML partnerships and Federation SSO with third party applications\\n\\n•\\tHands on with building custom adapters using Java SDK\\n•\\tImplementing rule based web SSO for enterprise users\\n•\\tExperience with PKI, X.509 certificates,SSL/TLS\\n•\\tExperience on following: \\n           o Identity & Access Management and/or federation experience \\n           o SAML,OAuth,OpenID,SOAP, Kerberos.\\n\\nExperience in IAM  field especially in engineering.\\n•\\tInstallation and conf  of PingAccess and PingFederate in a cluster environment\\n•\\tComplex policy conf of PingAccess and PingFederate products for implementing the WAM solution\\n•\\tIdentity federation configuration experience including STS using PingFederate\\n•\\tTroubleshooting knowledge of PingAccess and PingFederate.\\n•\\tJava development experience for customization of PingAccess and PingFederate.\\n\\nReplaced/upgraded Oracle Access Manager & CA siteminder infrastructure\\n- Built out federation infrastructure with vendor product (PingIdentity PingFederate 6.x)\\n- Designed and developed an extensible web app using PingFederate technology (Open Token API); migrated over differenent different homegrown federation integrations onto the PingFederate-integrated web application solution\\n- Designed and developed an extensible single log on (SLO) authentication web portal\\n- Designed and developed a flexible and configurable identity data exchange platform in Java using LDAP\\n\\n- Standardized and implemented operational procedures: documentation, version control, monitoring, backup, support', 'location': None, 'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T104859Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=f5c09b01354219061deec91194a7b567fc778b2de37df860c76a821834f0ca7b'}, {'starts_at': {'day': 1, 'month': 2, 'year': 2008}, 'ends_at': None, 'company': 'LinuxWorld Informatics Pvt Ltd', 'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/', 'title': 'Sr Devops Principal Consultant, Integrator, Architect and Corporate Trainer', 'description': 'Devops Principal Consultant, Architect, Integrator and Corporate Trainer\\n – Dell EMC, Wipro, TCS, WellsFargo, JP Morgan Chase, GE, CA technology, many more\\n\\n\\n•\\tIntegrate and provide consulting with large client base to devops chef tools for build design to testing tools with BDD and TDD approach like serverspec,chefspec,inspec,rspec, foodcritic,rubocop,berkshelf integrate with test kitchen with driver base of ec2, vagrant, docker\\n•\\tImplement chef using planned cookbook, roles, environment and by creating custom resource (LWRP2), library with encrypted and data bag vault\\n•\\tDeploy Ansible devops tools with ansible container and tower and write playbook for cassadra, bigdata hadoop and spark infrastructure.\\n•\\tTuned ansible play with facts and resources and create custom fact and resource module\\n•\\tBuild large infrastructure with Puppet devops tools with dynamic node classifier with ENC\\n•\\tImplement Puppet Hiera and create custom facts and modules and optimized classes and manifest file\\n•\\tHave working experience with managing Micro Services implementing using  Docker \\n•\\tImplement system deployment & management systems (Chef/Puppet/Ansible/Salt)\\n•\\tImplement complete workflow with Devops developer tools – GIT/svn/perforce/AWS codecommit, CodeHub, Jira, Artifactory/ nexus sonatype, Grunt, Gulp with Java and other build tools - Gradle, Ant, Maven, Rake\\n•\\tIntegrate CI/CD pipeline using Jenkins/Hudson/teamcity according to client needs that complete  code workflow management with AWS codepipeline / Gitlab with CI – continuous inspection server sonarqube\\n•\\tImplement workflow management with automate dollar universe\\n•\\tIntegrate Monitoring and reporting tools for e.g. Nagios, New Relic with devops automate workflow and artifact and apps business transition monitoring with APM tools appdynamics\\n•\\tImplementing  WebServers and LoadBalancers Apache HTTP Server, Apache Traffic Server, Nginx, HAProxy with Jboss/tomcat and integrate elastic search,logstach and kibaba stack (ELK)', 'location': None, 'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T104859Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=f5c09b01354219061deec91194a7b567fc778b2de37df860c76a821834f0ca7b'}, {'starts_at': {'day': 1, 'month': 10, 'year': 2007}, 'ends_at': None, 'company': 'LinuxWorld Informatics Pvt Ltd', 'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/', 'title': 'Sr AWS Cloud Principal Consultant, Integrator, Architect and Corporate Trainer', 'description': 'AWS Cloud Principal Consultant, Architect, Integrator and Corporate Trainer\\n – Samsung R&D, Dell EMC, Wipro,JP Morgan Chase, GE, CA technology, many more\\n\\n•\\tAutomating infrastructure using AWS Opsworks for configuration mngt\\n•\\tConfigure Event driven architecture using Lambda service\\n•\\tImplement Scalable and microservices over Elastic beanstalk\\n•\\tIntegrate Redis from Elasticache with onboarding apps\\n•\\tImplement  Amazon Web Services (AWS) extensively in a large AWS cross-account environment  IAM and federated SSO  PingFederate\\n•\\tTuning and Scaling Service with alerting, monitoring optimize uptime.\\n•\\tManaged AWS services with SDK and Create Multitier stack integrated with SQS and Configuration management and automation experience with Ansible,Python Boto\\n•\\tExperience with databases such as Oracle, Mysql, Postgres, DynamoDB and Redshift\\n•\\tProduction experience with Docker and Implemented through ECS services\\n•\\tStrong Experience with All the AWS components like EC2, ELB, Auto Scaling, launch Configurations, S3, Glacier, Lifecycle rules for storage, VPC, Route 53, Cloud formation,Cloud watch, Cloud Trial,Volume,Code Pipeline, SNS, IAM and Roles\\n•\\tArchitect of public and private Workload and integrate onboarding application with Openstack and  cloud platforms (AWS), Data migration on AWS\\n•\\tRedesigned infrastructure for high availability using multiple AWS availability zones using puppet\\n•\\tImplement Complete pipeline on DevOps using  Puppet\\n•\\tSetup all AWS services with  AWS CLI & APIs and integrate with Mobile Apps \\n•\\tImplementing Splunk Cluster on AWS and Experienced in monitoring health check\\n•\\tRun Bigdata Related Apps and Workload on AWS with batch processing using EMR and real time streaming integrate with SPARK and Kinesis\\n•\\tConsulting in transforming on-premise solutions to cloud and Design solutions for Private, Hybrid and Public models and business cases can articulate on the TCO and ROI and defined architecture design methodologies such as TOGAF, Zachman', 'location': None, 'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T104859Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=f5c09b01354219061deec91194a7b567fc778b2de37df860c76a821834f0ca7b'}, {'starts_at': {'day': 1, 'month': 10, 'year': 2002}, 'ends_at': None, 'company': 'LinuxWorld Informatics Pvt Ltd', 'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/', 'title': 'Chief Technical Officer (CTO)', 'description': 'Core responsibilities included managing scientific and technological issues within the organisation for enhanced operational efficiency, reduced process-related barriers and improved performance achievement. \\n\\nExecutive Roles: \\n\\nIdentifying the new technologies and making these vital part of organisational success\\nLeveraging on the benefits of technologies across business units\\nDriving business strategy into workable practices\\nDriving the learning modules of the company \\nDriving revenues from functional objective\\nCollaborate and communicate with client \\nReducing process-related obstacles\\n\\nOthers:\\n\\nParticipating in Linux and open source-based seminars and workshops across India \\nAs a speaker at National & International Conferences on various technologies namely Big Data Analytics, Cloud Computing, Hadoop Framework - The Need of the Day for Data Analysis, Modern programming languages namely R programming, Python, Perl and many more..\\nContributing to authority Linux and Open Source publications\\n\\nMiddleware Project :\\nRed Hat Jboss Data Virtualization to Integrate multiple data sources with diff. Modelling  and create federation with optimized performance and expose each services with RESTful API\\n\\nHave Done Altova MAPFORCE  for XML creations and generate dynamic code using map force\\nWorking with flat and other file format and mapping with database in MapForce\\n\\nGuest lectures in institutions', 'location': 'Jaipur Area, India', 'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T104859Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=f5c09b01354219061deec91194a7b567fc778b2de37df860c76a821834f0ca7b'}], 'education': [], 'languages': ['English', 'Hindi'], 'accomplishment_organisations': [], 'accomplishment_publications': [], 'accomplishment_honors_awards': [{'title': 'Invited as Judge of Grand Finale of Smart India Hackathon 2019 - Central Govt Initiative', 'issuer': 'India Central Govt', 'issued_on': {'day': 1, 'month': 3, 'year': 2019}, 'description': 'Invited as Judge of Grand Finale of Smart India Hackathon 2019 - Central Govt Initiative\\n\\nInaugurated by Shri Prakash Javdekar (Minister - HRD India) with physical presence at Jaipur and at the same time addressed by Shri Narendra Modiji (live conference)\\n\\nWorked with Ministry of Power dignitaries...To judge the problem statement given by Government\\n'}, {'title': 'FIRST PERSON IN THE WORLD who has acquired the Title of REDHAT CERTIFIED ARCHITECT LEVEL 22 WITH REDHAT CERTIFIED ARCHITECT ENTERPRISE APPLICATION LEVEL 7', 'issuer': None, 'issued_on': {'day': 1, 'month': 2, 'year': 2019}, 'description': 'I Created History - A World Record....\\n\\nA proud moment for me, PROUDEST moment for India.. I am humbled & proud to announce that I have become the FIRST PERSON IN THE WORLD who has acquired the Title of REDHAT CERTIFIED ARCHITECT LEVEL 22 WITH REDHAT CERTIFIED ARCHITECT ENTERPRISE APPLICATION LEVEL 7\\n\\nAll this is dedicated to my Country, my Students & my Family\\n\\nA giant leap towards \"Making India , Future Ready\".. The Ones who adapt with Change is successful but THE ONES WHO CAUSE THE CHANGE LEAD IN TECHNOLOGY...A message to Our Country, a message to the World, the GENIUS INSIDE YOU IS ALL SET TO COME OUT, JUST SHOW HIM /HER THE RIGHT PATH...\\n\\nAs I always quote \"I am working towards making India and we Indians as CREATOR OF TECHNOLOGIES, to do so WE SHOULD BE AHEAD FROM REST OF THE WORLD and So did I\"\\n\\nThanks everyone for your extended support'}, {'title': 'Awarded as \"Distinguish Alumni Award\" for distinguished achievements in his field — at JECRC Foundation', 'issuer': None, 'issued_on': None, 'description': None}, {'title': 'LinuxWorld awarded as the \"Best Internship and Maximum Growth Partner in entire Asia Pacific and India\" by RedHat Global Head Mr. Randy Russell and APAC Head Mr. Vikram Motiani @ Hotel The Taj Mahal Palace, Mumbai', 'issuer': 'RedHat', 'issued_on': None, 'description': 'Unbelievable 2 Awards in 1 go @ RHTPC, 2018 at Mumbai\\n\\nLinuxWorld awarded as the \"Best Internship and Maximum Growth Partner in entire Asia Pacific and India\" by RedHat Global Head Mr. Randy Russell and APAC Head Mr. Vikram Motiani @ Hotel The Taj Mahal Palace, Mumbai...\\n\\nWe won 2 awards one after the another...Entire RedHat Team was talking about our achievements we made during Summer Internship 2018...\\n\\nThe entire credit goes to our students, it wouldn\\'t have been possible without their unconditional love and support..\\n\\nThanks to RedHat and LW team'}], 'accomplishment_patents': [], 'accomplishment_courses': [], 'accomplishment_projects': [{'starts_at': None, 'ends_at': None, 'title': 'BigData – Data Analysis Platform', 'description': \"•\\tSetup Complete Big Data Hadoop Technology of Multi Node Cluster \\n•\\tManage Each and Every Component of big data for distributed storage HDFS, distributed computing MapReduce , Distributed Scheduling YARN \\n•\\tWorks on Big Data Hadoop Related Framework like PIG, HIVE, HBase, Oozie, Zookeeper, Flume , Sqoop, Splunk many more \\n•\\tDeveloped Own Web Application to implement big data hadoop infrastructure at one click with YARN Resource Manager & Managed Hadoop 2 Resource Manager Using Mesos \\n•\\tDeployed Many Processing Engine and Apps Over Spark Framework \\n•\\tWorked on Real Streaming Processing on Storm API and integrated with Kafka MQ \\n•\\tDeveloped lot of business logics on Scala Platform using Spark API \\n•\\tMachine Learning using Mahout \\n•\\tWorked and Deployed NoSQL and Cluster Based database System like HBase, Cassandra, couch-base, Dynamo DB, Maria DB to manage unstructured or Semi –structured data \\n•\\tDevelop Data Analysis and Mining Project using R & SAS programming \\n•\\tCloud Based data analysis tools – Amazon EMR \\n•\\tElasticsearch Architect and Implementation Specialist, Logstash and Kibana Implementation Specialist.\\n•\\tInstallation, configuration and maintenance of Elasticsearch Linux server based system and network applications.\\nWorked on Azure/AWS Cloud for setting up ES Cluster for API's.\\nFixing Bugs related to ES.\\nWriting ES Query for Development.\\nOptimizing the performance of Elastic Search server.\\nPreparation of ES Settings and Mapping for Elastic Search Indices.\\nBulk loading data Using Curl commands.\\nUpgrading & Maintenance of ES cluster.\\nWorked in shell scripts for automatic bulk load.\\n• Built high-throughput and fault-tolerant river to pre-compute and pipe MongoDB, and RabbitMQ data into Elasticsearch\\n• Built a tool which simplies the creation of complex nested Elasticsearch queries\\n• Transformed data from Apache Spark into a schema optimized for complex analytics in Elasticsearch\", 'url': None}, {'starts_at': None, 'ends_at': None, 'title': 'Cloud ComputingTechnologies – Deployment, Testing & Security', 'description': '•\\tManaging and Configuring Virtualization Technologies Over the Hypervisor Xen, KVM, EXSi \\n•\\tWorks and and Managed Full RedHat Virtualization Technology Product RHEVM, with complete migration and datacenter Configuration \\n•\\tWorks on Virtualization Products Like VirtualBox, VMware Workstation, Virt-Manager \\n•\\tDevelop Over own Virtualization Frontend Over KVM using Python API \\n•\\tFully Configure and Manage Amazon AWS Cloud for Huge Topology \\n•\\tSet Each and Every Amazon AWS Products like EC2, VPC, ELB, S3, Glacier, EMR, DynamoDB, EBS,RDS, IAM, many more \\n•\\tWorks at Architect Level on OpenStack Cloud and Configure for Public and Private Architect \\n•\\tManaging and Implementing Each Stack of Openstack like Nova, Cinder, Glance, Keystone, Horizon, Qpid, Swift, Neutron/Quantum \\n•\\tSetup Multi Node Environment of OpenStack Over VLAN, GRE tunnel \\n•\\tConfigure Distributed Cloud Storage using GlusterFS filesystem \\n•\\tDeploy our OWN cloud and Architect each Service by own code using Python like Saas, Staas, Naas, Paas, Iaas \\n•\\tCloud PaaS – RedHat OpenShift & Google App Engine \\n•\\tCloud IaaS – RedHat OpenStack, AWS, Microsoft Azure & Google Compute Engine \\n•\\tManaging and Configuring Message Queue in distributed application using Apache Qpid, RabbitMQ, Apache AMQ, Amazon SQS \\n•\\tSetup Advanced Message Queuing Protocol (AMQP) for Middleware interoperability \\n', 'url': None}, {'starts_at': None, 'ends_at': None, 'title': 'DevOps & Containers Automation Technologies', 'description': '•\\tManage and Implement Project Build Tools using Apache Maven & Nexus Sonatype Repository Manager \\n•\\tManaging DevOps Platform using Chef, Ansible, CF Engine and Puppet for building Continuous automated delivery services \\n•\\tExperience in Building and Troubleshooting Continuous Integration CI using Hudson/ Jenkins \\n•\\tWork on Revision and Source Control System Perforce / GIT / SVN \\n•\\tWorked on Containerized Technologies viz Dockers, Rocket, Kubernetes, Swarm, Magnum, Amazon Container Service, RedHat Atomic Host & Core OS. \\n', 'url': None}, {'starts_at': None, 'ends_at': None, 'title': 'Programming Languages', 'description': '•\\tHas Done High-End Automation scripting in Bash, Perl & Ruby \\n•\\tWorked on other languages namely Python, PHP, Java Script, Scala, Go\\n', 'url': None}, {'starts_at': None, 'ends_at': None, 'title': 'RedHat & Linux Based Technologies', 'description': '•\\tWrite SELinux custom policy for enterprise level application with their daemon type transition, port binding, boolean to control security feature \\n•\\tManage and configure MLS and MCS using SELinux for high security driven platform for enterprise \\n•\\tWorked on docker container MCS separation by svirt and enabled customize enforcing type of different container \\n•\\tManaging Linux System Administrator Task like Performance Tuning, Kernel Re- compiling, MBR and GPT parameter tuning, Format understaning like ext2/ext3/ext3/xfs, Partition managing like swap, raid,lvm, runlevel till rhel6 version and Systemd management from RHEL7,many more \\n•\\tConfiguring Linux Server on RHEL server like telnet, openssh, ftp/sftp/ftps, http/https, samba, nfs, dns,dhcp, nis,smtp sendmail, smtp postfix, ldap,ntp, tftp, many more \\n•\\tImplement and Test Security on Linux System like SELinux, Firewall – Iptables / Firewalld, ACL, Implement Kerberos Server and AAA server – TACACS/ Radius \\n', 'url': None}, {'starts_at': None, 'ends_at': None, 'title': 'Security Implement & Pen Security - Web Application Technologies – Development', 'description': \"•\\tImplement Ping Identity's PingFederate Single Sign On (SSO) solution for centralized management and audit of user program access. Users simply log on to a PingFederate portal, then access a pre-selected list of applications without need to re-authenticate.\\nPingFederate, Security, Oauth 2.0, e-business security, SAML, Social Login, Identity providers - Google, Facebook, Digidentity, SSO, Expert opinions, Security scans, Federated Idenity & Access management, Cloud Identity\\n•\\tAudit and Testing Web Bugs and Analyse Security Vulnerability like SQL-Injection, XSS, CSRF, Cookie Manipulation, Buffer Overflow, Many more in Web Application. \\n•\\tPenetration Testing Risk Management Network Security Code Audits, Malware Analysis \\n•\\tVulnerability Assessment Business Continuity Forensic Investigation Exploit Development \\n•\\tSpecialized experience in Information Assurance and security program management, risk management, network and systems penetration testing and vulnerability assessment \\n•\\tProficient in the testing and analysis of benchmarking the application against industry standards such as OWASP ASVS, Top 10 and SAN \\n•\\tExperience on IDS/IPS like Snort, OSSEC and implementation of VPN tunnels over IPSec. Experience in implementing and managing IPSec tunnels across different production network \\n•\\tCisco SourceFire Managed Devices - Using FireSIGHT Defence Center, NGIPS Sensors & ASA FirePOWER\\n•\\tApplication security assessment of numerous web applications and mobile applications using Professional Burp Suite, Acunetix,etc. \\n•\\tIntegrating Web services Architecture API using SOAP and REST to parse web data \\n•\\tConfigure Web Server Like Apache Tomcat, Apache HTTPD, nginx with SSL, Proxy, Authentication Support and Tuned or Optimized for Performance and Load Analysis \\n•\\tImplement Infrastructure Monitoring tools like Nagios and Tuned Some internal SNMP Feature for Extra Support and Connect to External API for Reporting \\n\\n\", 'url': None}], 'accomplishment_test_scores': [], 'volunteer_work': [], 'certifications': [{'starts_at': None, 'ends_at': None, 'name': 'AWS Certified Big Data - Specialty', 'license_number': '3KZFXFBCGJEE18GR', 'display_source': None, 'authority': 'Amazon Web Services (AWS)', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'AWS Certified Machine Learning – Specialty', 'license_number': '1489E5BKMM4EQEGG', 'display_source': None, 'authority': 'Amazon Web Services (AWS)', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'AWS Certified Solutions Architect - Associate 2019', 'license_number': '0STNT1T1EBQQ1G3S', 'display_source': None, 'authority': 'Amazon Web Services (AWS)', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Amazon Web Services Certified Developer Associate', 'license_number': None, 'display_source': None, 'authority': 'Amazon Web Services (AWS)', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Amazon Web Services Certified Solutions Associate', 'license_number': None, 'display_source': None, 'authority': 'Amazon Web Services (AWS)', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Amazon Web Services Certified SysOps Associate', 'license_number': None, 'display_source': None, 'authority': 'Amazon Web Services (AWS)', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Certified Blockchain Expert', 'license_number': None, 'display_source': None, 'authority': 'Blockchain Council', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Certified Information Systems Security Professional (CISSP)', 'license_number': None, 'display_source': None, 'authority': None, 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Chef DevOps Cookbook Development Badge Certification', 'license_number': None, 'display_source': None, 'authority': 'Progress Chef', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Cisco Certified Internetwork Expert Routing n Switching (CCIE R&S) *', 'license_number': None, 'display_source': None, 'authority': 'Cisco', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Cisco Certified Internetwork Expert Security (CCIE Security) *', 'license_number': None, 'display_source': None, 'authority': 'Cisco', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Cisco Certified Network Associate (CCNA - R&S and Security)', 'license_number': None, 'display_source': None, 'authority': 'Cisco', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Cisco Certified Network Professional (CCNP - R&S and Security)', 'license_number': None, 'display_source': None, 'authority': 'Cisco', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Cisco Certified System Instructor (CCSI)', 'license_number': None, 'display_source': None, 'authority': 'Cisco', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Cloudera Certified Administrator for Apache Hadoop (CCAH)', 'license_number': None, 'display_source': None, 'authority': 'Cloudera', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Computer Hacking Forensic Investigator (CHFI)', 'license_number': None, 'display_source': None, 'authority': 'EC-Council', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'EC- Council Certified Ethical Hacker - V7', 'license_number': None, 'display_source': None, 'authority': None, 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'EC-Council Certified Instructor', 'license_number': None, 'display_source': None, 'authority': None, 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'EC-Council Certified Security Analyst (ECSA)', 'license_number': None, 'display_source': None, 'authority': None, 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Google cloud Platform Associate Cloud Engineer', 'license_number': None, 'display_source': None, 'authority': 'Google', 'url': None}], 'connections': None, 'people_also_viewed': [{'link': 'https://www.linkedin.com/in/rahul-cse', 'name': 'Rahul Maheshwari', 'summary': 'Er-Adobe, Ex-Google | Founder - LinuxSocials | Philanthropist | Speaker', 'location': None}, {'link': 'https://www.linkedin.com/in/preeti-chandak', 'name': 'Preeti Chandak', 'summary': 'Chief Strategy Officer at LinuxWorld Informatics Pvt Ltd | Training Co-Ordinator (Corporate & Retail -IT) | Entrepreneur', 'location': None}, {'link': 'https://www.linkedin.com/in/shrishti-kapoor', 'name': 'Shrishti Kapoor', 'summary': 'FTE @Cognizant | 2 x RedHat Certified | Aviatrix Certified Engineer | AI-900 | Technical Trainer | IIEC-DOT Volunteer | Technical Research Writer Enthusiast..!! 🔷DevOps 🔷Cloud-AWS, Azure 🔷ML/AI/DL', 'location': None}, {'link': 'https://www.linkedin.com/in/priyansh-magotra-a98282141', 'name': 'Priyansh Magotra', 'summary': 'TSE@Red Hat | x6 Red Hat Certified | OpenShift Support Delivery | Kubernetes | DevOps (Docker/Podman/CRI-O/Buildah | Ansible | Git | Jenkins) | RedHat Linux', 'location': None}, {'link': 'https://www.linkedin.com/in/saiyampathak', 'name': 'Saiyam Pathak', 'summary': 'Director of Technical Evangelism at Civo | CNCF Ambassador | Author | Founder @kubesimplify', 'location': None}, {'link': 'https://www.linkedin.com/in/rohit-ghumare', 'name': 'Rohit Ghumare', 'summary': 'DevRel🥑 @solo.io | Founder @Keep Up(20K+)💙 | R&D🔍 | Public Speaker🎙️| Community Builder @AWS☁️ | DevOps, Data Science, MLOps | Technical Teaching Assistant | Kubernetes, Istio & Rasa Official Contributor', 'location': None}, {'link': 'https://www.linkedin.com/in/raktimmidya', 'name': 'Raktim Midya', 'summary': 'Aspiring Freelancer & DevOps Consultant ★ Taught Hundreds of Students & Still Teaching ★1x AWS, 5x RedHat, 4x Microsoft Certified ★ AWS Community Builder ★ Technical Content Creator @Medium @YouTube', 'location': None}, {'link': 'https://www.linkedin.com/in/dipadityadas', 'name': 'Dipaditya Das', 'summary': 'Aspiring Software Development Engineer || 4x Red Hat Certified || 7x Microsoft Certified || AWS Community Builders || Google Cloud Facilitator || Technical Volunteer at ARTH || Open Source Contributor', 'location': None}, {'link': 'https://www.linkedin.com/in/sonikaushal', 'name': 'Kaushal Soni', 'summary': '🔔 Youtuber @DevopsGyan ➤ 2xRedHat Certified ➤ GCP Certified ➤ Cloud Engineer @ Searce ➤ DevOps & AWS Instructor ➤ Experience in DevOps (🐳☸👩🏻\\u200d🍳), AWS☁️ & ML 🧠 \\xa0Stack', 'location': None}, {'link': 'https://www.linkedin.com/in/20ashisharma', 'name': 'Ashi Sharma', 'summary': 'I want to be CREATOR of Technology | Learn to Lead | DevOps | Cloud | Arthlearner | Ansible | Docker | Kubernetes | Jenkins | AWS | Linux | ML', 'location': None}], 'recommendations': [], 'activities': [], 'similarly_named_profiles': [{'name': 'Vimal Daga', 'link': 'https://in.linkedin.com/in/vimal-daga-9219931ba', 'summary': '--', 'location': 'Sehore'}, {'name': 'vimal daga', 'link': 'https://in.linkedin.com/in/vimal-daga-9288a232', 'summary': 'Owner of Rishabh international star trading house', 'location': 'Jodhpur'}, {'name': 'Vimal Daga', 'link': 'https://in.linkedin.com/in/vimal-daga-5467a8108', 'summary': 'CEO at SSJ', 'location': 'Mumbai'}, {'name': 'Vimal Daga', 'link': 'https://in.linkedin.com/in/vimal-daga-46847a9b', 'summary': 'Food Production Professional', 'location': 'Raipur'}], 'articles': [], 'groups': [], 'phone_numbers': [], 'social_networking_services': [{'service': 'github', 'canonical_url': 'github.com/vimallinuxworld13', 'internal_id': None}], 'skills': ['Linux', 'Cloud Computing', 'Big Data', 'OpenStack', 'Dev Ops', 'Splunk', 'Ping Federate', 'Docker', 'Ansible', 'Chef', 'Big Data Analytics', 'Storage - Glusterfs', 'Apache Hadoop', 'R Programming', 'Amazon Web Services (AWS)', 'Enterprise Network Security', 'Web Application Security', 'Web Application Security Assessment', 'Perl', 'Cisco Technologies'], 'inferred_salary': {'min': None, 'max': None}, 'gender': None, 'birth_date': None, 'industry': None, 'extra': {'github_profile_id': None, 'twitter_profile_id': None, 'facebook_profile_id': None}, 'interests': [], 'personal_emails': ['vdaga@lwindia.com'], 'personal_numbers': []}\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens. However, your messages resulted in 13170 tokens. Please reduce the length of the messages.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[142], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmyagent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmyprompttemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43msomeoneinformation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlw_scrape_linkedin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://www.linkedin.com/in/vimaldaga/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msummary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                  \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chains\\base.py:256\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chains\\base.py:145\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    144\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    146\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    147\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    148\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    149\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chains\\base.py:139\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[0;32m    133\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    134\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    135\u001b[0m     inputs,\n\u001b[0;32m    136\u001b[0m )\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    138\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    142\u001b[0m     )\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    144\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\agents\\agent.py:953\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m--> 953\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m    961\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m    962\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m    963\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\agents\\agent.py:762\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;124;03m\"\"\"Take a single step in the thought-action-observation loop.\u001b[39;00m\n\u001b[0;32m    757\u001b[0m \n\u001b[0;32m    758\u001b[0m \u001b[38;5;124;03mOverride this to take control of how the agent makes and acts on choices.\u001b[39;00m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    761\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m--> 762\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mplan(\n\u001b[0;32m    763\u001b[0m         intermediate_steps,\n\u001b[0;32m    764\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    765\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m    766\u001b[0m     )\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\agents\\agent.py:443\u001b[0m, in \u001b[0;36mAgent.plan\u001b[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m\"\"\"Given input, decided what to do.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;124;03m    Action specifying what tool to use.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    442\u001b[0m full_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_full_inputs(intermediate_steps, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 443\u001b[0m full_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfull_inputs)\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_parser\u001b[38;5;241m.\u001b[39mparse(full_output)\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chains\\llm.py:213\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chains\\base.py:145\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    144\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    146\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    147\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    148\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    149\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chains\\base.py:139\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[0;32m    133\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    134\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    135\u001b[0m     inputs,\n\u001b[0;32m    136\u001b[0m )\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    138\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    142\u001b[0m     )\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    144\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chains\\llm.py:69\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     66\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m     67\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     68\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m---> 69\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chains\\llm.py:79\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[0;32m     78\u001b[0m prompts, stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_prompts(input_list, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m     81\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chat_models\\base.py:148\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    143\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m    144\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    145\u001b[0m     callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    146\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    147\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chat_models\\base.py:92\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     91\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     93\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n\u001b[0;32m     94\u001b[0m generations \u001b[38;5;241m=\u001b[39m [res\u001b[38;5;241m.\u001b[39mgenerations \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results]\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chat_models\\base.py:84\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks)\u001b[0m\n\u001b[0;32m     80\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     82\u001b[0m )\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(m, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(m, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[0;32m     89\u001b[0m     ]\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     91\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chat_models\\base.py:85\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     80\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     82\u001b[0m )\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 85\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(m, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[0;32m     89\u001b[0m     ]\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     91\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chat_models\\openai.py:323\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager)\u001b[0m\n\u001b[0;32m    319\u001b[0m     message \u001b[38;5;241m=\u001b[39m _convert_dict_to_message(\n\u001b[0;32m    320\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: inner_completion, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: role}\n\u001b[0;32m    321\u001b[0m     )\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[ChatGeneration(message\u001b[38;5;241m=\u001b[39mmessage)])\n\u001b[1;32m--> 323\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_with_retry(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chat_models\\openai.py:284\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _completion_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\tenacity\\__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_explicit_retry \u001b[38;5;241m=\u001b[39m fut\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fut\u001b[38;5;241m.\u001b[39mexception(), TryAgain)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry(retry_state)):\n\u001b[1;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter(retry_state)\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chat_models\\openai.py:282\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\openai\\api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    761\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 763\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[0;32m    764\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[0;32m    765\u001b[0m     )\n\u001b[0;32m    766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens. However, your messages resulted in 13170 tokens. Please reduce the length of the messages."
     ]
    }
   ],
   "source": [
    "myagent.run(\n",
    "    myprompttemplate.format_prompt( \n",
    "        \n",
    "someoneinformation=lw_scrape_linkedin('https://www.linkedin.com/in/vimaldaga/')[\"summary\"]\n",
    "                                  \n",
    "                                  ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d67c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa506563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c58968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210328a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b354e29f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2a01f79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d783490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_for_agent1 = [\n",
    "        Tool(\n",
    "            name=\"Crawl Vimal daga linkedin profile page\",\n",
    "            func=lw_scrape_linkedin,\n",
    "            description=\"useful for when you need get the Linkedin Page URL\",\n",
    "        ),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "57f29a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(\n",
    "        tools_for_agent1, \n",
    "    model, \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c3d4bf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should extract the required information from the given text about Vimal Daga's LinkedIn profile.\n",
      "Action: Crawl Vimal daga linkedin profile page\n",
      "Action Input: None\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'public_identifier': 'vimaldaga', 'profile_pic_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/person/vimaldaga/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T102346Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=c2cd9ebf371a22cad3fc542d2521f3e6ad684c2ce9080aa690d9fb0819a42745', 'background_cover_image_url': None, 'first_name': 'Vimal', 'last_name': 'Daga', 'full_name': 'Vimal Daga', 'follower_count': None, 'occupation': 'Sr. Machine Learning / Deep Learning / Data Scientist / NLP Consultant and Researcher at LinuxWorld Informatics Pvt Ltd', 'headline': 'World Record Holder | TEDx Speaker | Philanthropist | Sr. Principal Consultant | Entrepreneur | Founder LW Informatics', 'summary': \"Aloha, I am Vimal Daga, known as an Technologist & also a Technology Motivational Speaker, Sr. IT Consultant & Corporate Trainer having Expertize in Latest and High-End Technologies like Machine Learning, Artificial Intelligence, Deep Learning,IoT, NLP, Splunk, PingFederate, Delphix, AppDynamics, Docker, DevOps, AWS, Cloud Computing, Big Data Analytics Dollar Universe..\\n\\nEmail: vdaga@lwindia.com\\n\\nI am an IT Enthusiast, who is passionate about exploring all the latest technologies from research perspective. I inspire, train and provide Consulting Services to all the companies willing to work, migrate or use high end IT Technologies and also help MNC's to find the right approach to get best ROI.\\n◆ MY BELIEF\\nIn my own words – “No technology is challenging or difficult or complex as the world says to us or approaches to us since it is MAN-MADE. You just need the right path or approach to understand the technology and take it further as per one’s requirement”. Right thinking or positive thinking that “Yes I Can Do It” can change one’s life drastically.\\n\\n◆ MY WHAT\\nI train and provide consultancy to all the Top Level Management including the Directors, CTO’s, Project Leads and also Senior IT executives working on technologies or willing to understand whether the new technology which they are willing to adapt is the right one. It helps them in enhancing their technical skills and also ability to handle complex project based environment.\\n\\n◆ MY TECHNOLOGIES\\n•      Machine Learning, AI, DL, NLP\\n•\\tDevOps – Chef, Puppet, Ansible, Jenkins, Docker\\n•\\tOps Intelligence & Other Tools – Splunk, PingFederate, Delphix, AppDynamics\\n•\\tCloud Computing – AWS, OpenStack, \\n•\\tBigData Analytics – Hadoop, Spark, Mahout, Cassandra\\n•\\tRedHat, Cisco, Security and Many more\\n\\n◆ MY SUCCESSFUL CLIENTS\\nWells Fargo, JP Morgan Chase, Bank of America, GE, CA Technologies, Dell, TCS, Deloitte, PwC, HP, Wipro, IBM, Samsung R&D, Ericsson and many more…\\n\\nLET'S CONNECT\\nI am easy to reach M-F at 0091.982.912.5960\", 'country': 'IN', 'country_full_name': 'India', 'city': 'Bengaluru', 'state': 'Karnataka', 'experiences': [{'starts_at': {'day': 1, 'month': 10, 'year': 2009}, 'ends_at': None, 'company': 'LinuxWorld Informatics Pvt. Ltd.', 'company_linkedin_profile_url': None, 'title': 'Sr. Splunk Consultant and Corporate Trainer', 'description': 'Splunk Consulting, Architect, SIEM, Administration, Developing and Training \\n – PwC, WellsFargo, JP Morgan Chase, Deloitte,  many more\\n\\nMore Than 4 years experience in Splunk Entreprise. Deploying large data analytics architectures for customers mainly in the Banking section, Governments and Security companies. Consult and Solutions to Over 30 Splunk projects and over 40 Corporate Batch Trained\\n\\nIntegrating Splunk with Arcsight and Splunk for Security and Risk Analysis including Information Asset Management and SIEM.\\n\\n•       Architecture design and implementation of Splunk solution\\n•       Platform optimization\\n•       Design and implement indexers and search heads Clusters (+migrations)\\n•\\tImplementing distributed Splunk and ES\\n•\\tConfiguring Splunk heavy forwarders to receive logs using push or pull\\n•\\tWriting complex correlation searches for data analysis and notable events\\n•\\tWriting optimized searches for realtime and historical data analysis\\n•\\tWriting custom apps for field extractions and data normalization\\n•\\tDeveloping and deploying customized configuration apps for distributed Splunk\\n•\\tWriting field extractions for desired fields out of proprietary application data\\n•\\tCreating reports and correlated alerts for stake holders about application behavior and anomalies\\n•\\tData aggregation and summarization by summary indexing and acceleration \\n•\\tWriting transaction searches and data normalization\\n•\\tIntegrating Splunk with AD servers for ldap authentications or SAML PING FEDERATE, DBs for enrichments, Hadoop servers for parallel processing using Hadoop Connect\\n•\\tUpgrading stand-alone Splunk to distributed Splunk implementation\\n•\\tWriting custom inputs, props and transforms to have correct event-breaks, field extractions and transformations\\n•\\tStrategizing and writing custom configurations for data routing and selective indexing for the location wise forwarders\\n•\\tDeveloping interactive dashboards, reports and optimized search queries for users', 'location': None, 'logo_url': None}, {'starts_at': {'day': 1, 'month': 4, 'year': 2014}, 'ends_at': None, 'company': 'LinuxWorld Informatics Pvt Ltd', 'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/', 'title': 'Microservices Architect and Corporate Trainer', 'description': '•\\tMicroservices Spring boot/Spring cloud, Dropwizard\\n•\\tExpertise in Java/ J2EE, Microservices , Spring Boot, Spring Cloud, EJB, terms of designing and implementing systems.\\n•\\tmodel and build scalable Microservices components/ APIs \\n•\\tmodel object for no sql DB \\n•\\tmodel and build integration flows on various API development platform \\n•\\tperform pair programming \\n•\\tbuild test scripts for Microservices components \\n•\\tDecompose an existing monolithic into microservices archtecture \\n•\\tcreated microservices architecture for new development \\n•\\tDefine, develop and manage API lifecycle or PaaS \\n•\\tDefine, develop components compliant wth Microservices architecture patterns \\n•\\tDefine and model cloud based persistence storage \\n•\\tIntegrate with cloud based storage, on-prem applcations / integration layer \\n•\\tDefine, develop runtime models for Microservices architecture \\n•\\tDefine, develop highly scalable deployment model for Microservices\\n•\\tImplement microservices on Docker Containerized environment', 'location': None, 'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T102350Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=6e9490ac3bc456ac9473eb512e0a848ceab427460e985de2a0ba73f060e0bc72'}, {'starts_at': {'day': 1, 'month': 2, 'year': 2014}, 'ends_at': None, 'company': 'LinuxWorld Informatics Pvt Ltd', 'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/', 'title': 'Appdynamics Consultant and Trainer', 'description': 'AWS Cloud Principal Consultant, Architect, Integrator and Corporate Trainer\\n– WellsFargo, JP Morgan Chase  many more\\n\\n•\\tManaging and Monitoring various metrics to analyze performance of application hosted on PaaS platform using Appdynamics for microservices\\n•\\tMonitor multi-tier architecture using AppDynamics Platform deployed on Cloud\\n•\\tMonitoring slow queries and finding bottlenecks\\n•\\tInstall App Agent & Machine agent\\n•\\tComplex Application monitoring in Appdynamics \\n•\\tCreate Application Dashboard and manage Transaction Snapshot / Scorecard and Diagnostic session\\n•\\tCodelevel issue repot and Monitor and Analyze\\n•\\tConfiguring Dashboards and Charting Data and Alert Log / Troubleshooting\\n•\\tCreating and Managing Reports and Portal Administration\\n•\\tAccount Details and User Management in appdynamics', 'location': None, 'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T102350Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=6e9490ac3bc456ac9473eb512e0a848ceab427460e985de2a0ba73f060e0bc72'}, {'starts_at': {'day': 1, 'month': 10, 'year': 2013}, 'ends_at': None, 'company': 'LinuxWorld Informatics Pvt Ltd', 'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/', 'title': 'Sr. Docker CI/CD Architect, Consultant and Corporate Trainer', 'description': 'Dcoker Container Consulting, Architect, Administration and Training \\n – Samsung R&D, Dell EMC, Verizon, NTT,  GE, CA Technology, Oracle, IBM, Amazon, Wipro, TCS, many more\\n\\nTraining and Consulting to Team Leads, CoE, Sr. Engineers for container related technologies like Docker with Cluster and Orchestration Engine Swarm, Kubernetes, Mesosphere and entire CI/CD cycle with containers\\nTrained Cloud DevOps Engineer to raise the bar for a high-performing team that delivers large scale solutions on the cloud. Fast-moving organization to transform a large enterprise onto the next generation of Cloud.\\n\\nAutomating large scale deployments with modern configuration and deployment management systems on AWS using Docker Containers Environment with ECS and managed through Code Pipeline.\\nDelivered container based deployments using Docker and can offer solutions to the complexities of Orchestration and will drive the company towards delivering containers in production.\\n\\n•\\tExtensive experience in Continuous Integration tool Bamboo/Jenkins/Circle CI with Integration with Docker\\n•\\tExperience in automation of the deployment and configuration of the virtualized infrastructure and the entire software stack and network stack through Docker\\n•\\tExtensive experience in build configurations using Build Management tool Docker for applications in various technologies including Python, Ruby, Scala, Java and many more\\n•\\tWorked and Trained on Docker Compose to auto High-End build pipeline\\n\\nExpert on Ansible/ Chef / Puppet to provisioning Docker task and in working with scripting Languages to configuring and maintaining CI/CD environment and automate AGILE Projects', 'location': None, 'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T102350Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=6e9490ac3bc456ac9473eb512e0a848ceab427460e985de2a0ba73f060e0bc72'}, {'starts_at': {'day': 1, 'month': 2, 'year': 2013}, 'ends_at': None, 'company': 'LinuxWorld Informatics Pvt Ltd', 'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/', 'title': 'Centrify IDaaS Consultant, Integrator and Corporate Trainer', 'description': 'Centrify Principal Consultant, Architect, Integrator and Corporate Trainer\\n–  IBM and many more\\n\\nIntegrated and Implement  Centrify identity management, security, SaaS single sign-on, Identity as a Service (IDaaS), mobile device management, mobile authentication, privileged access management, multi-factor authentication (MFA), privileged identity management, and identity and access management\\n\\nImplement POC of Centrify Server Suite with multiple vendor with different use cases', 'location': None, 'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T102350Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=6e9490ac3bc456ac9473eb512e0a848ceab427460e985de2a0ba73f060e0bc72'}, {'starts_at': {'day': 1, 'month': 3, 'year': 2012}, 'ends_at': None, 'company': 'LinuxWorld Informatics Pvt Ltd', 'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/', 'title': 'Automic Dollar Universe Consultant and Corporate Trainer', 'description': 'Dollar Universe, AWA Consultant, Architect, Integrator and Corporate Trainer\\n –  IBM, Wipro, TCS\\n\\n•\\tProvide scheduling product support for CA Autosys, Tidal. Consult on BMC Control-M and ORSYP Dollar Universe\\n•\\tI have administrative experience with Automic product.\\n•\\tSetup multiple UVMS cluster with DUAS active/ passive mode in dollar universe , integration with LDAP/ SSO product\\n•\\tConfigure complete workflow and pipeline to integrate with AWS cloud in automic product \\n•\\tRecently provided consulting services to migrate CA Autosys workloads to BMC Control-M for Unix and Windows at Baxter. Provided scheduling migration technique, provided requirements and migration plan, executed conversion of Autosys job files to Control-M scheduling format. Completed Proof of Concept and Phase one efforts for Autosys to Control-M migration.\\n•\\tIntegrating and managing cluster of $U and also write custom plugin as per our client use cases\\no\\tArchitecture of $Universe with parameters of $Universe and Logs management and logs retention of $Universe.\\no\\tData/Objects retrieval methods of $Universe and Performance optimization required for $Universe administration with Product patch installation.\\no\\tCoordinate with vendor in resolving product issues with Alerts integration with monitoring tools$Universe job scheduling\\no\\tAdding/modifying/updating of jobs on $Universe.\\no\\tScheduling of jobs in $universe for applications like SAP and integrate with docker, AWS\\no\\tFixing of job failures and debugging skills of job failures.\\no\\tAction ad hoc request on active job scheduling environment', 'location': None, 'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T102350Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=6e9490ac3bc456ac9473eb512e0a848ceab427460e985de2a0ba73f060e0bc72'}, {'starts_at': {'day': 1, 'month': 2, 'year': 2012}, 'ends_at': None, 'company': 'LinuxWorld Informatics Pvt Ltd', 'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/', 'title': 'Sr. Openstack Cloud Consultant and Corporate Trainer', 'description': 'Design, build, maintain and operate one of the largest open stack based cloud deployments with all the component,  Nova, Glance, Swift, Horizon, Keystone, Neutron, Cinder, Heat, Ceilometer, Trove, Sahara, Ironic, Zaqar, Manila, Designate, Barbican, Searchlight, Magnum, aodh, cloudkitty, congress, freezer, mistral, monasca-api, monasca-log-api, murano, panko, senlin, solum, tacker, vitrage, Watcher\\n\\nCapacity management, and provisioning. Maintaining and troubleshooting\\n\\nsetup and manage a multi-node Openstack environment with docker implement and provision with Devops tools\\n\\nDesigns, builds, and manages enterprise Redhat RHOSP and Helion OpenStack (HOS) Cloud environments. That involve creating capacity collection and monitoring capabilities, upgrade projects, customer consultations, operations, and design enhancements\\n\\nHave works as Openstack Consultant and also has strong knowledge of SDN, database design, networking and storage.', 'location': None, 'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T102350Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=6e9490ac3bc456ac9473eb512e0a848ceab427460e985de2a0ba73f060e0bc72'}, {'starts_at': {'day': 1, 'month': 10, 'year': 2010}, 'ends_at': None, 'company': 'LinuxWorld Informatics Pvt Ltd', 'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/', 'title': 'Sr. Machine Learning / Deep Learning / Data Scientist / NLP Consultant and Researcher', 'description': 'Consulting, Architect, Developing and Training \\n – PwC, WellsFargo, RBS, GE , Oracle , many more\\n\\n-> Expertise in the field of Artificial Intelligence, Deep Learning, and Computer Vision and having ability to solve problems such as Face Detection, Face Recognition and Object Detection using Deep Neural Network  (CNN, DNN, RNN, Convolution Networks etc.) and Optical Character Detection and Recognition (OCD & OCR)\\n\\n-> Worked in tools such as Tensorflow, Caffe/Caffe2, Keras, Theano, PyTorch etc.\\n\\n-> Build prototypes related to deep learning problems in the field of computer vision.\\n\\n-> Publications at top international conferences/ journals in fields related to computer vision/deep learning/machine learning / AI\\n\\n-> Experience on  tools, frameworks like Microsoft Azure ML, Chat Bot Framework/LUIS . IBM Watson / ConversationService, Google TensorFlow / Python for Machine Learning (e.g. scikit-learn),Open source ML libraries and tools like Apache Spark\\nHighly Worked on  Data Science, Big Data,datastructures, statistics , algorithms like Regression, Classification etc.\\n\\n-> Working knowlegde of Supervised / Unsuperivsed learning (Decision Trees, Logistic Regression, SVMs,GBM, etc)\\n\\n-> Expertise in Sentiment Analysis, Entity Extraction, Natural Language Understanding (NLU), Intent recognition\\n\\n-> Strong understanding of text pre-processing and normalization techniques, such as tokenization, POS tagging, and parsing, and how they work at a basic level and NLP toolkits as NLTK, Gensim,, Apac SpaCyhe UIMA etc.\\n\\n-> I have Hands on experience related to Datasets such as or including text, images and other logs or clickstreams. and have worked on text data cleaning, parsing and web scraping with regular expressions for text extraction and cleansing\\n\\n->Worked on recommendation algorithm used in e-commerce and advertising and  have knowledge of applied machine learning to solve real-world problems and Applied unsupervised, supervised, and semi-supervised ML in own problem domain', 'location': None, 'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T102350Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=6e9490ac3bc456ac9473eb512e0a848ceab427460e985de2a0ba73f060e0bc72'}, {'starts_at': {'day': 1, 'month': 10, 'year': 2009}, 'ends_at': None, 'company': 'LinuxWorld Informatics Pvt Ltd', 'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/', 'title': 'Sr. Ping Identity, Ping Federation and Ping Access Consultant and Corporate Trainer', 'description': 'Ping Identity, Ping Federation and Ping Access Consulting, Architect, Administration and Training \\n – IBM, Wipro, WellsFargo, many more\\n\\n• Hands on experience on implementing web SSO solution using Ping Access & Ping Federation products from PingIdentity\\n• Consult to Drive and lead IAM Security projects for the organization\\n• Subject Matter Expert in MFA / SSO / Ping Federate / SAML  / OAuth 2.0 / OpenID Connect(OIDC)\\n• Managed Corporate Ping Federate Environment for SAML partnerships and Federation SSO with third party applications\\n\\n•\\tHands on with building custom adapters using Java SDK\\n•\\tImplementing rule based web SSO for enterprise users\\n•\\tExperience with PKI, X.509 certificates,SSL/TLS\\n•\\tExperience on following: \\n           o Identity & Access Management and/or federation experience \\n           o SAML,OAuth,OpenID,SOAP, Kerberos.\\n\\nExperience in IAM  field especially in engineering.\\n•\\tInstallation and conf  of PingAccess and PingFederate in a cluster environment\\n•\\tComplex policy conf of PingAccess and PingFederate products for implementing the WAM solution\\n•\\tIdentity federation configuration experience including STS using PingFederate\\n•\\tTroubleshooting knowledge of PingAccess and PingFederate.\\n•\\tJava development experience for customization of PingAccess and PingFederate.\\n\\nReplaced/upgraded Oracle Access Manager & CA siteminder infrastructure\\n- Built out federation infrastructure with vendor product (PingIdentity PingFederate 6.x)\\n- Designed and developed an extensible web app using PingFederate technology (Open Token API); migrated over differenent different homegrown federation integrations onto the PingFederate-integrated web application solution\\n- Designed and developed an extensible single log on (SLO) authentication web portal\\n- Designed and developed a flexible and configurable identity data exchange platform in Java using LDAP\\n\\n- Standardized and implemented operational procedures: documentation, version control, monitoring, backup, support', 'location': None, 'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T102350Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=6e9490ac3bc456ac9473eb512e0a848ceab427460e985de2a0ba73f060e0bc72'}, {'starts_at': {'day': 1, 'month': 2, 'year': 2008}, 'ends_at': None, 'company': 'LinuxWorld Informatics Pvt Ltd', 'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/', 'title': 'Sr Devops Principal Consultant, Integrator, Architect and Corporate Trainer', 'description': 'Devops Principal Consultant, Architect, Integrator and Corporate Trainer\\n – Dell EMC, Wipro, TCS, WellsFargo, JP Morgan Chase, GE, CA technology, many more\\n\\n\\n•\\tIntegrate and provide consulting with large client base to devops chef tools for build design to testing tools with BDD and TDD approach like serverspec,chefspec,inspec,rspec, foodcritic,rubocop,berkshelf integrate with test kitchen with driver base of ec2, vagrant, docker\\n•\\tImplement chef using planned cookbook, roles, environment and by creating custom resource (LWRP2), library with encrypted and data bag vault\\n•\\tDeploy Ansible devops tools with ansible container and tower and write playbook for cassadra, bigdata hadoop and spark infrastructure.\\n•\\tTuned ansible play with facts and resources and create custom fact and resource module\\n•\\tBuild large infrastructure with Puppet devops tools with dynamic node classifier with ENC\\n•\\tImplement Puppet Hiera and create custom facts and modules and optimized classes and manifest file\\n•\\tHave working experience with managing Micro Services implementing using  Docker \\n•\\tImplement system deployment & management systems (Chef/Puppet/Ansible/Salt)\\n•\\tImplement complete workflow with Devops developer tools – GIT/svn/perforce/AWS codecommit, CodeHub, Jira, Artifactory/ nexus sonatype, Grunt, Gulp with Java and other build tools - Gradle, Ant, Maven, Rake\\n•\\tIntegrate CI/CD pipeline using Jenkins/Hudson/teamcity according to client needs that complete  code workflow management with AWS codepipeline / Gitlab with CI – continuous inspection server sonarqube\\n•\\tImplement workflow management with automate dollar universe\\n•\\tIntegrate Monitoring and reporting tools for e.g. Nagios, New Relic with devops automate workflow and artifact and apps business transition monitoring with APM tools appdynamics\\n•\\tImplementing  WebServers and LoadBalancers Apache HTTP Server, Apache Traffic Server, Nginx, HAProxy with Jboss/tomcat and integrate elastic search,logstach and kibaba stack (ELK)', 'location': None, 'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T102350Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=6e9490ac3bc456ac9473eb512e0a848ceab427460e985de2a0ba73f060e0bc72'}, {'starts_at': {'day': 1, 'month': 10, 'year': 2007}, 'ends_at': None, 'company': 'LinuxWorld Informatics Pvt Ltd', 'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/', 'title': 'Sr AWS Cloud Principal Consultant, Integrator, Architect and Corporate Trainer', 'description': 'AWS Cloud Principal Consultant, Architect, Integrator and Corporate Trainer\\n – Samsung R&D, Dell EMC, Wipro,JP Morgan Chase, GE, CA technology, many more\\n\\n•\\tAutomating infrastructure using AWS Opsworks for configuration mngt\\n•\\tConfigure Event driven architecture using Lambda service\\n•\\tImplement Scalable and microservices over Elastic beanstalk\\n•\\tIntegrate Redis from Elasticache with onboarding apps\\n•\\tImplement  Amazon Web Services (AWS) extensively in a large AWS cross-account environment  IAM and federated SSO  PingFederate\\n•\\tTuning and Scaling Service with alerting, monitoring optimize uptime.\\n•\\tManaged AWS services with SDK and Create Multitier stack integrated with SQS and Configuration management and automation experience with Ansible,Python Boto\\n•\\tExperience with databases such as Oracle, Mysql, Postgres, DynamoDB and Redshift\\n•\\tProduction experience with Docker and Implemented through ECS services\\n•\\tStrong Experience with All the AWS components like EC2, ELB, Auto Scaling, launch Configurations, S3, Glacier, Lifecycle rules for storage, VPC, Route 53, Cloud formation,Cloud watch, Cloud Trial,Volume,Code Pipeline, SNS, IAM and Roles\\n•\\tArchitect of public and private Workload and integrate onboarding application with Openstack and  cloud platforms (AWS), Data migration on AWS\\n•\\tRedesigned infrastructure for high availability using multiple AWS availability zones using puppet\\n•\\tImplement Complete pipeline on DevOps using  Puppet\\n•\\tSetup all AWS services with  AWS CLI & APIs and integrate with Mobile Apps \\n•\\tImplementing Splunk Cluster on AWS and Experienced in monitoring health check\\n•\\tRun Bigdata Related Apps and Workload on AWS with batch processing using EMR and real time streaming integrate with SPARK and Kinesis\\n•\\tConsulting in transforming on-premise solutions to cloud and Design solutions for Private, Hybrid and Public models and business cases can articulate on the TCO and ROI and defined architecture design methodologies such as TOGAF, Zachman', 'location': None, 'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T102350Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=6e9490ac3bc456ac9473eb512e0a848ceab427460e985de2a0ba73f060e0bc72'}, {'starts_at': {'day': 1, 'month': 10, 'year': 2002}, 'ends_at': None, 'company': 'LinuxWorld Informatics Pvt Ltd', 'company_linkedin_profile_url': 'https://www.linkedin.com/company/linuxworld-informatics-pvt-ltd/', 'title': 'Chief Technical Officer (CTO)', 'description': 'Core responsibilities included managing scientific and technological issues within the organisation for enhanced operational efficiency, reduced process-related barriers and improved performance achievement. \\n\\nExecutive Roles: \\n\\nIdentifying the new technologies and making these vital part of organisational success\\nLeveraging on the benefits of technologies across business units\\nDriving business strategy into workable practices\\nDriving the learning modules of the company \\nDriving revenues from functional objective\\nCollaborate and communicate with client \\nReducing process-related obstacles\\n\\nOthers:\\n\\nParticipating in Linux and open source-based seminars and workshops across India \\nAs a speaker at National & International Conferences on various technologies namely Big Data Analytics, Cloud Computing, Hadoop Framework - The Need of the Day for Data Analysis, Modern programming languages namely R programming, Python, Perl and many more..\\nContributing to authority Linux and Open Source publications\\n\\nMiddleware Project :\\nRed Hat Jboss Data Virtualization to Integrate multiple data sources with diff. Modelling  and create federation with optimized performance and expose each services with RESTful API\\n\\nHave Done Altova MAPFORCE  for XML creations and generate dynamic code using map force\\nWorking with flat and other file format and mapping with database in MapForce\\n\\nGuest lectures in institutions', 'location': 'Jaipur Area, India', 'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/linuxworld-informatics-pvt-ltd/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20230716%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20230716T102350Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=6e9490ac3bc456ac9473eb512e0a848ceab427460e985de2a0ba73f060e0bc72'}], 'education': [], 'languages': ['English', 'Hindi'], 'accomplishment_organisations': [], 'accomplishment_publications': [], 'accomplishment_honors_awards': [{'title': 'Invited as Judge of Grand Finale of Smart India Hackathon 2019 - Central Govt Initiative', 'issuer': 'India Central Govt', 'issued_on': {'day': 1, 'month': 3, 'year': 2019}, 'description': 'Invited as Judge of Grand Finale of Smart India Hackathon 2019 - Central Govt Initiative\\n\\nInaugurated by Shri Prakash Javdekar (Minister - HRD India) with physical presence at Jaipur and at the same time addressed by Shri Narendra Modiji (live conference)\\n\\nWorked with Ministry of Power dignitaries...To judge the problem statement given by Government\\n'}, {'title': 'FIRST PERSON IN THE WORLD who has acquired the Title of REDHAT CERTIFIED ARCHITECT LEVEL 22 WITH REDHAT CERTIFIED ARCHITECT ENTERPRISE APPLICATION LEVEL 7', 'issuer': None, 'issued_on': {'day': 1, 'month': 2, 'year': 2019}, 'description': 'I Created History - A World Record....\\n\\nA proud moment for me, PROUDEST moment for India.. I am humbled & proud to announce that I have become the FIRST PERSON IN THE WORLD who has acquired the Title of REDHAT CERTIFIED ARCHITECT LEVEL 22 WITH REDHAT CERTIFIED ARCHITECT ENTERPRISE APPLICATION LEVEL 7\\n\\nAll this is dedicated to my Country, my Students & my Family\\n\\nA giant leap towards \"Making India , Future Ready\".. The Ones who adapt with Change is successful but THE ONES WHO CAUSE THE CHANGE LEAD IN TECHNOLOGY...A message to Our Country, a message to the World, the GENIUS INSIDE YOU IS ALL SET TO COME OUT, JUST SHOW HIM /HER THE RIGHT PATH...\\n\\nAs I always quote \"I am working towards making India and we Indians as CREATOR OF TECHNOLOGIES, to do so WE SHOULD BE AHEAD FROM REST OF THE WORLD and So did I\"\\n\\nThanks everyone for your extended support'}, {'title': 'Awarded as \"Distinguish Alumni Award\" for distinguished achievements in his field — at JECRC Foundation', 'issuer': None, 'issued_on': None, 'description': None}, {'title': 'LinuxWorld awarded as the \"Best Internship and Maximum Growth Partner in entire Asia Pacific and India\" by RedHat Global Head Mr. Randy Russell and APAC Head Mr. Vikram Motiani @ Hotel The Taj Mahal Palace, Mumbai', 'issuer': 'RedHat', 'issued_on': None, 'description': 'Unbelievable 2 Awards in 1 go @ RHTPC, 2018 at Mumbai\\n\\nLinuxWorld awarded as the \"Best Internship and Maximum Growth Partner in entire Asia Pacific and India\" by RedHat Global Head Mr. Randy Russell and APAC Head Mr. Vikram Motiani @ Hotel The Taj Mahal Palace, Mumbai...\\n\\nWe won 2 awards one after the another...Entire RedHat Team was talking about our achievements we made during Summer Internship 2018...\\n\\nThe entire credit goes to our students, it wouldn\\'t have been possible without their unconditional love and support..\\n\\nThanks to RedHat and LW team'}], 'accomplishment_patents': [], 'accomplishment_courses': [], 'accomplishment_projects': [{'starts_at': None, 'ends_at': None, 'title': 'BigData – Data Analysis Platform', 'description': \"•\\tSetup Complete Big Data Hadoop Technology of Multi Node Cluster \\n•\\tManage Each and Every Component of big data for distributed storage HDFS, distributed computing MapReduce , Distributed Scheduling YARN \\n•\\tWorks on Big Data Hadoop Related Framework like PIG, HIVE, HBase, Oozie, Zookeeper, Flume , Sqoop, Splunk many more \\n•\\tDeveloped Own Web Application to implement big data hadoop infrastructure at one click with YARN Resource Manager & Managed Hadoop 2 Resource Manager Using Mesos \\n•\\tDeployed Many Processing Engine and Apps Over Spark Framework \\n•\\tWorked on Real Streaming Processing on Storm API and integrated with Kafka MQ \\n•\\tDeveloped lot of business logics on Scala Platform using Spark API \\n•\\tMachine Learning using Mahout \\n•\\tWorked and Deployed NoSQL and Cluster Based database System like HBase, Cassandra, couch-base, Dynamo DB, Maria DB to manage unstructured or Semi –structured data \\n•\\tDevelop Data Analysis and Mining Project using R & SAS programming \\n•\\tCloud Based data analysis tools – Amazon EMR \\n•\\tElasticsearch Architect and Implementation Specialist, Logstash and Kibana Implementation Specialist.\\n•\\tInstallation, configuration and maintenance of Elasticsearch Linux server based system and network applications.\\nWorked on Azure/AWS Cloud for setting up ES Cluster for API's.\\nFixing Bugs related to ES.\\nWriting ES Query for Development.\\nOptimizing the performance of Elastic Search server.\\nPreparation of ES Settings and Mapping for Elastic Search Indices.\\nBulk loading data Using Curl commands.\\nUpgrading & Maintenance of ES cluster.\\nWorked in shell scripts for automatic bulk load.\\n• Built high-throughput and fault-tolerant river to pre-compute and pipe MongoDB, and RabbitMQ data into Elasticsearch\\n• Built a tool which simplies the creation of complex nested Elasticsearch queries\\n• Transformed data from Apache Spark into a schema optimized for complex analytics in Elasticsearch\", 'url': None}, {'starts_at': None, 'ends_at': None, 'title': 'Cloud ComputingTechnologies – Deployment, Testing & Security', 'description': '•\\tManaging and Configuring Virtualization Technologies Over the Hypervisor Xen, KVM, EXSi \\n•\\tWorks and and Managed Full RedHat Virtualization Technology Product RHEVM, with complete migration and datacenter Configuration \\n•\\tWorks on Virtualization Products Like VirtualBox, VMware Workstation, Virt-Manager \\n•\\tDevelop Over own Virtualization Frontend Over KVM using Python API \\n•\\tFully Configure and Manage Amazon AWS Cloud for Huge Topology \\n•\\tSet Each and Every Amazon AWS Products like EC2, VPC, ELB, S3, Glacier, EMR, DynamoDB, EBS,RDS, IAM, many more \\n•\\tWorks at Architect Level on OpenStack Cloud and Configure for Public and Private Architect \\n•\\tManaging and Implementing Each Stack of Openstack like Nova, Cinder, Glance, Keystone, Horizon, Qpid, Swift, Neutron/Quantum \\n•\\tSetup Multi Node Environment of OpenStack Over VLAN, GRE tunnel \\n•\\tConfigure Distributed Cloud Storage using GlusterFS filesystem \\n•\\tDeploy our OWN cloud and Architect each Service by own code using Python like Saas, Staas, Naas, Paas, Iaas \\n•\\tCloud PaaS – RedHat OpenShift & Google App Engine \\n•\\tCloud IaaS – RedHat OpenStack, AWS, Microsoft Azure & Google Compute Engine \\n•\\tManaging and Configuring Message Queue in distributed application using Apache Qpid, RabbitMQ, Apache AMQ, Amazon SQS \\n•\\tSetup Advanced Message Queuing Protocol (AMQP) for Middleware interoperability \\n', 'url': None}, {'starts_at': None, 'ends_at': None, 'title': 'DevOps & Containers Automation Technologies', 'description': '•\\tManage and Implement Project Build Tools using Apache Maven & Nexus Sonatype Repository Manager \\n•\\tManaging DevOps Platform using Chef, Ansible, CF Engine and Puppet for building Continuous automated delivery services \\n•\\tExperience in Building and Troubleshooting Continuous Integration CI using Hudson/ Jenkins \\n•\\tWork on Revision and Source Control System Perforce / GIT / SVN \\n•\\tWorked on Containerized Technologies viz Dockers, Rocket, Kubernetes, Swarm, Magnum, Amazon Container Service, RedHat Atomic Host & Core OS. \\n', 'url': None}, {'starts_at': None, 'ends_at': None, 'title': 'Programming Languages', 'description': '•\\tHas Done High-End Automation scripting in Bash, Perl & Ruby \\n•\\tWorked on other languages namely Python, PHP, Java Script, Scala, Go\\n', 'url': None}, {'starts_at': None, 'ends_at': None, 'title': 'RedHat & Linux Based Technologies', 'description': '•\\tWrite SELinux custom policy for enterprise level application with their daemon type transition, port binding, boolean to control security feature \\n•\\tManage and configure MLS and MCS using SELinux for high security driven platform for enterprise \\n•\\tWorked on docker container MCS separation by svirt and enabled customize enforcing type of different container \\n•\\tManaging Linux System Administrator Task like Performance Tuning, Kernel Re- compiling, MBR and GPT parameter tuning, Format understaning like ext2/ext3/ext3/xfs, Partition managing like swap, raid,lvm, runlevel till rhel6 version and Systemd management from RHEL7,many more \\n•\\tConfiguring Linux Server on RHEL server like telnet, openssh, ftp/sftp/ftps, http/https, samba, nfs, dns,dhcp, nis,smtp sendmail, smtp postfix, ldap,ntp, tftp, many more \\n•\\tImplement and Test Security on Linux System like SELinux, Firewall – Iptables / Firewalld, ACL, Implement Kerberos Server and AAA server – TACACS/ Radius \\n', 'url': None}, {'starts_at': None, 'ends_at': None, 'title': 'Security Implement & Pen Security - Web Application Technologies – Development', 'description': \"•\\tImplement Ping Identity's PingFederate Single Sign On (SSO) solution for centralized management and audit of user program access. Users simply log on to a PingFederate portal, then access a pre-selected list of applications without need to re-authenticate.\\nPingFederate, Security, Oauth 2.0, e-business security, SAML, Social Login, Identity providers - Google, Facebook, Digidentity, SSO, Expert opinions, Security scans, Federated Idenity & Access management, Cloud Identity\\n•\\tAudit and Testing Web Bugs and Analyse Security Vulnerability like SQL-Injection, XSS, CSRF, Cookie Manipulation, Buffer Overflow, Many more in Web Application. \\n•\\tPenetration Testing Risk Management Network Security Code Audits, Malware Analysis \\n•\\tVulnerability Assessment Business Continuity Forensic Investigation Exploit Development \\n•\\tSpecialized experience in Information Assurance and security program management, risk management, network and systems penetration testing and vulnerability assessment \\n•\\tProficient in the testing and analysis of benchmarking the application against industry standards such as OWASP ASVS, Top 10 and SAN \\n•\\tExperience on IDS/IPS like Snort, OSSEC and implementation of VPN tunnels over IPSec. Experience in implementing and managing IPSec tunnels across different production network \\n•\\tCisco SourceFire Managed Devices - Using FireSIGHT Defence Center, NGIPS Sensors & ASA FirePOWER\\n•\\tApplication security assessment of numerous web applications and mobile applications using Professional Burp Suite, Acunetix,etc. \\n•\\tIntegrating Web services Architecture API using SOAP and REST to parse web data \\n•\\tConfigure Web Server Like Apache Tomcat, Apache HTTPD, nginx with SSL, Proxy, Authentication Support and Tuned or Optimized for Performance and Load Analysis \\n•\\tImplement Infrastructure Monitoring tools like Nagios and Tuned Some internal SNMP Feature for Extra Support and Connect to External API for Reporting \\n\\n\", 'url': None}], 'accomplishment_test_scores': [], 'volunteer_work': [], 'certifications': [{'starts_at': None, 'ends_at': None, 'name': 'AWS Certified Big Data - Specialty', 'license_number': '3KZFXFBCGJEE18GR', 'display_source': None, 'authority': 'Amazon Web Services (AWS)', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'AWS Certified Machine Learning – Specialty', 'license_number': '1489E5BKMM4EQEGG', 'display_source': None, 'authority': 'Amazon Web Services (AWS)', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'AWS Certified Solutions Architect - Associate 2019', 'license_number': '0STNT1T1EBQQ1G3S', 'display_source': None, 'authority': 'Amazon Web Services (AWS)', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Amazon Web Services Certified Developer Associate', 'license_number': None, 'display_source': None, 'authority': 'Amazon Web Services (AWS)', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Amazon Web Services Certified Solutions Associate', 'license_number': None, 'display_source': None, 'authority': 'Amazon Web Services (AWS)', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Amazon Web Services Certified SysOps Associate', 'license_number': None, 'display_source': None, 'authority': 'Amazon Web Services (AWS)', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Certified Blockchain Expert', 'license_number': None, 'display_source': None, 'authority': 'Blockchain Council', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Certified Information Systems Security Professional (CISSP)', 'license_number': None, 'display_source': None, 'authority': None, 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Chef DevOps Cookbook Development Badge Certification', 'license_number': None, 'display_source': None, 'authority': 'Progress Chef', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Cisco Certified Internetwork Expert Routing n Switching (CCIE R&S) *', 'license_number': None, 'display_source': None, 'authority': 'Cisco', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Cisco Certified Internetwork Expert Security (CCIE Security) *', 'license_number': None, 'display_source': None, 'authority': 'Cisco', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Cisco Certified Network Associate (CCNA - R&S and Security)', 'license_number': None, 'display_source': None, 'authority': 'Cisco', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Cisco Certified Network Professional (CCNP - R&S and Security)', 'license_number': None, 'display_source': None, 'authority': 'Cisco', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Cisco Certified System Instructor (CCSI)', 'license_number': None, 'display_source': None, 'authority': 'Cisco', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Cloudera Certified Administrator for Apache Hadoop (CCAH)', 'license_number': None, 'display_source': None, 'authority': 'Cloudera', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Computer Hacking Forensic Investigator (CHFI)', 'license_number': None, 'display_source': None, 'authority': 'EC-Council', 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'EC- Council Certified Ethical Hacker - V7', 'license_number': None, 'display_source': None, 'authority': None, 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'EC-Council Certified Instructor', 'license_number': None, 'display_source': None, 'authority': None, 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'EC-Council Certified Security Analyst (ECSA)', 'license_number': None, 'display_source': None, 'authority': None, 'url': None}, {'starts_at': None, 'ends_at': None, 'name': 'Google cloud Platform Associate Cloud Engineer', 'license_number': None, 'display_source': None, 'authority': 'Google', 'url': None}], 'connections': None, 'people_also_viewed': [{'link': 'https://www.linkedin.com/in/rahul-cse', 'name': 'Rahul Maheshwari', 'summary': 'Er-Adobe, Ex-Google | Founder - LinuxSocials | Philanthropist | Speaker', 'location': None}, {'link': 'https://www.linkedin.com/in/preeti-chandak', 'name': 'Preeti Chandak', 'summary': 'Chief Strategy Officer at LinuxWorld Informatics Pvt Ltd | Training Co-Ordinator (Corporate & Retail -IT) | Entrepreneur', 'location': None}, {'link': 'https://www.linkedin.com/in/shrishti-kapoor', 'name': 'Shrishti Kapoor', 'summary': 'FTE @Cognizant | 2 x RedHat Certified | Aviatrix Certified Engineer | AI-900 | Technical Trainer | IIEC-DOT Volunteer | Technical Research Writer Enthusiast..!! 🔷DevOps 🔷Cloud-AWS, Azure 🔷ML/AI/DL', 'location': None}, {'link': 'https://www.linkedin.com/in/priyansh-magotra-a98282141', 'name': 'Priyansh Magotra', 'summary': 'TSE@Red Hat | x6 Red Hat Certified | OpenShift Support Delivery | Kubernetes | DevOps (Docker/Podman/CRI-O/Buildah | Ansible | Git | Jenkins) | RedHat Linux', 'location': None}, {'link': 'https://www.linkedin.com/in/saiyampathak', 'name': 'Saiyam Pathak', 'summary': 'Director of Technical Evangelism at Civo | CNCF Ambassador | Author | Founder @kubesimplify', 'location': None}, {'link': 'https://www.linkedin.com/in/rohit-ghumare', 'name': 'Rohit Ghumare', 'summary': 'DevRel🥑 @solo.io | Founder @Keep Up(20K+)💙 | R&D🔍 | Public Speaker🎙️| Community Builder @AWS☁️ | DevOps, Data Science, MLOps | Technical Teaching Assistant | Kubernetes, Istio & Rasa Official Contributor', 'location': None}, {'link': 'https://www.linkedin.com/in/raktimmidya', 'name': 'Raktim Midya', 'summary': 'Aspiring Freelancer & DevOps Consultant ★ Taught Hundreds of Students & Still Teaching ★1x AWS, 5x RedHat, 4x Microsoft Certified ★ AWS Community Builder ★ Technical Content Creator @Medium @YouTube', 'location': None}, {'link': 'https://www.linkedin.com/in/dipadityadas', 'name': 'Dipaditya Das', 'summary': 'Aspiring Software Development Engineer || 4x Red Hat Certified || 7x Microsoft Certified || AWS Community Builders || Google Cloud Facilitator || Technical Volunteer at ARTH || Open Source Contributor', 'location': None}, {'link': 'https://www.linkedin.com/in/sonikaushal', 'name': 'Kaushal Soni', 'summary': '🔔 Youtuber @DevopsGyan ➤ 2xRedHat Certified ➤ GCP Certified ➤ Cloud Engineer @ Searce ➤ DevOps & AWS Instructor ➤ Experience in DevOps (🐳☸👩🏻\\u200d🍳), AWS☁️ & ML 🧠 \\xa0Stack', 'location': None}, {'link': 'https://www.linkedin.com/in/20ashisharma', 'name': 'Ashi Sharma', 'summary': 'I want to be CREATOR of Technology | Learn to Lead | DevOps | Cloud | Arthlearner | Ansible | Docker | Kubernetes | Jenkins | AWS | Linux | ML', 'location': None}], 'recommendations': [], 'activities': [], 'similarly_named_profiles': [{'name': 'Vimal Daga', 'link': 'https://in.linkedin.com/in/vimal-daga-9219931ba', 'summary': '--', 'location': 'Sehore'}, {'name': 'vimal daga', 'link': 'https://in.linkedin.com/in/vimal-daga-9288a232', 'summary': 'Owner of Rishabh international star trading house', 'location': 'Jodhpur'}, {'name': 'Vimal Daga', 'link': 'https://in.linkedin.com/in/vimal-daga-5467a8108', 'summary': 'CEO at SSJ', 'location': 'Mumbai'}, {'name': 'Vimal Daga', 'link': 'https://in.linkedin.com/in/vimal-daga-46847a9b', 'summary': 'Food Production Professional', 'location': 'Raipur'}], 'articles': [], 'groups': [], 'phone_numbers': [], 'social_networking_services': [{'service': 'github', 'canonical_url': 'github.com/vimallinuxworld13', 'internal_id': None}], 'skills': ['Linux', 'Cloud Computing', 'Big Data', 'OpenStack', 'Dev Ops', 'Splunk', 'Ping Federate', 'Docker', 'Ansible', 'Chef', 'Big Data Analytics', 'Storage - Glusterfs', 'Apache Hadoop', 'R Programming', 'Amazon Web Services (AWS)', 'Enterprise Network Security', 'Web Application Security', 'Web Application Security Assessment', 'Perl', 'Cisco Technologies'], 'inferred_salary': {'min': None, 'max': None}, 'gender': None, 'birth_date': None, 'industry': None, 'extra': {'github_profile_id': None, 'twitter_profile_id': None, 'facebook_profile_id': None}, 'interests': [], 'personal_emails': ['vdaga@lwindia.com'], 'personal_numbers': []}\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens. However, your messages resulted in 13176 tokens. Please reduce the length of the messages.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[131], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m linkedin_username \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmyprompttemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43msomeoneinformation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msummary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chains\\base.py:256\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chains\\base.py:145\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    144\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    146\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    147\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    148\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    149\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chains\\base.py:139\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[0;32m    133\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    134\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    135\u001b[0m     inputs,\n\u001b[0;32m    136\u001b[0m )\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    138\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    142\u001b[0m     )\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    144\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\agents\\agent.py:953\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m--> 953\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m    961\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m    962\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m    963\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\agents\\agent.py:762\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;124;03m\"\"\"Take a single step in the thought-action-observation loop.\u001b[39;00m\n\u001b[0;32m    757\u001b[0m \n\u001b[0;32m    758\u001b[0m \u001b[38;5;124;03mOverride this to take control of how the agent makes and acts on choices.\u001b[39;00m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    761\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m--> 762\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mplan(\n\u001b[0;32m    763\u001b[0m         intermediate_steps,\n\u001b[0;32m    764\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    765\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m    766\u001b[0m     )\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\agents\\agent.py:443\u001b[0m, in \u001b[0;36mAgent.plan\u001b[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m\"\"\"Given input, decided what to do.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;124;03m    Action specifying what tool to use.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    442\u001b[0m full_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_full_inputs(intermediate_steps, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 443\u001b[0m full_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfull_inputs)\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_parser\u001b[38;5;241m.\u001b[39mparse(full_output)\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chains\\llm.py:213\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chains\\base.py:145\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    144\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    146\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    147\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    148\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    149\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chains\\base.py:139\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[0;32m    133\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    134\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    135\u001b[0m     inputs,\n\u001b[0;32m    136\u001b[0m )\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    138\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    142\u001b[0m     )\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    144\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chains\\llm.py:69\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     66\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m     67\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     68\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m---> 69\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chains\\llm.py:79\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[0;32m     78\u001b[0m prompts, stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_prompts(input_list, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m     81\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chat_models\\base.py:148\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    143\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m    144\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    145\u001b[0m     callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    146\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    147\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chat_models\\base.py:92\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     91\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     93\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n\u001b[0;32m     94\u001b[0m generations \u001b[38;5;241m=\u001b[39m [res\u001b[38;5;241m.\u001b[39mgenerations \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results]\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chat_models\\base.py:84\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks)\u001b[0m\n\u001b[0;32m     80\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     82\u001b[0m )\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(m, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(m, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[0;32m     89\u001b[0m     ]\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     91\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chat_models\\base.py:85\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     80\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     82\u001b[0m )\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 85\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(m, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[0;32m     89\u001b[0m     ]\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     91\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chat_models\\openai.py:323\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager)\u001b[0m\n\u001b[0;32m    319\u001b[0m     message \u001b[38;5;241m=\u001b[39m _convert_dict_to_message(\n\u001b[0;32m    320\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: inner_completion, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: role}\n\u001b[0;32m    321\u001b[0m     )\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[ChatGeneration(message\u001b[38;5;241m=\u001b[39mmessage)])\n\u001b[1;32m--> 323\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_with_retry(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chat_models\\openai.py:284\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _completion_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\tenacity\\__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_explicit_retry \u001b[38;5;241m=\u001b[39m fut\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fut\u001b[38;5;241m.\u001b[39mexception(), TryAgain)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry(retry_state)):\n\u001b[1;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter(retry_state)\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\langchain\\chat_models\\openai.py:282\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3_new_2023\\lib\\site-packages\\openai\\api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    761\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 763\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[0;32m    764\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[0;32m    765\u001b[0m     )\n\u001b[0;32m    766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens. However, your messages resulted in 13176 tokens. Please reduce the length of the messages."
     ]
    }
   ],
   "source": [
    "linkedin_username = agent.run(\n",
    "    myprompttemplate.format_prompt(someoneinformation=data['summary']\n",
    "                                  )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6100cb55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
